<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://mtuanbui.github.io/site/feed.xml" rel="self" type="application/atom+xml"/><link href="https://mtuanbui.github.io/site/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-29T11:33:02+00:00</updated><id>https://mtuanbui.github.io/site/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">A Pytorch Lightning implementation of MNIST flow matching generative model</title><link href="https://mtuanbui.github.io/site/blog/2025/cfm-mnist/" rel="alternate" type="text/html" title="A Pytorch Lightning implementation of MNIST flow matching generative model"/><published>2025-04-05T00:00:00+00:00</published><updated>2025-04-05T00:00:00+00:00</updated><id>https://mtuanbui.github.io/site/blog/2025/cfm-mnist</id><content type="html" xml:base="https://mtuanbui.github.io/site/blog/2025/cfm-mnist/"><![CDATA[ <p>In this post, we will apply the theory of the Optimal Transport Flow Matching model, as introduced in the <a href="../cfm/">previous post</a>, to train a class-conditioned flow matching model for generating samples from the MNIST dataset distribution. We’ll use the PyTorch Lightning framework to avoid boilerplate code and focus solely on the core model logic.</p> <p>We use a UNet as the architecture for the flow matching model in this implementation. Since the main purpose is to demonstrate flow matching model training, we don’t focus on the neural network design and instead re-use the built-in UNet implementation from Diffusers library. The network architecture is similar to the one shown <a href="https://huggingface.co/learn/diffusion-course/en/unit2/3">here</a>, where class labels are encoded via an embedding layer, and the resulting class embeddings are concatenated to the UNet input along the channel dimension. We also modify <code class="language-plaintext highlighter-rouge">forward</code> function to support classifier-free guidance (CFG) training and utilize <code class="language-plaintext highlighter-rouge">einops</code> to reshape tensors for better clarity. The network implementation is kept as simple as the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">ConditionalUnet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">class_emb_size</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">class_emb</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">class_emb_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nc">UNet2DModel</span><span class="p">(</span>
            <span class="n">sample_size</span><span class="o">=</span><span class="mi">28</span><span class="p">,</span>
            <span class="n">in_channels</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">class_emb_size</span><span class="p">,</span>
            <span class="n">out_channels</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
            <span class="n">layers_per_block</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
            <span class="n">block_out_channels</span><span class="o">=</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
            <span class="n">down_block_types</span><span class="o">=</span><span class="p">(</span>
                <span class="sh">"</span><span class="s">DownBlock2D</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">AttnDownBlock2D</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">AttnDownBlock2D</span><span class="sh">"</span>
            <span class="p">),</span>
            <span class="n">up_block_types</span><span class="o">=</span><span class="p">(</span>
                <span class="sh">"</span><span class="s">AttnUpBlock2D</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">AttnUpBlock2D</span><span class="sh">"</span><span class="p">,</span>
                <span class="sh">"</span><span class="s">UpBlock2D</span><span class="sh">"</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">class_labels</span><span class="p">,</span> <span class="n">is_drop</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">bs</span><span class="p">,</span> <span class="n">ch</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>

        <span class="c1"># Get class embeddings
</span>        <span class="n">class_cond</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">class_emb</span><span class="p">(</span><span class="n">class_labels</span><span class="p">)</span>

        <span class="c1"># Drop conditions for CFG training
</span>        <span class="k">if</span> <span class="n">is_drop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">class_cond</span><span class="p">[</span><span class="n">is_drop</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">class_cond</span><span class="p">[</span><span class="n">is_drop</span><span class="p">])</span>

        <span class="c1"># Concatenate class embeddings to input x
</span>        <span class="n">class_cond</span> <span class="o">=</span> <span class="n">eio</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">class_cond</span><span class="p">,</span> <span class="sh">"</span><span class="s">bs c -&gt; bs c h w</span><span class="sh">"</span><span class="p">,</span> <span class="n">w</span><span class="o">=</span><span class="n">w</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">)</span>
        <span class="n">net_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">class_cond</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">net_input</span><span class="p">,</span> <span class="n">t</span><span class="p">).</span><span class="n">sample</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>We now proceed to implement model training with Pytorch Lightning. Here are some notes regarding this implementation.</p> <ul> <li>As an implementation of the Optimal Transport Flow model, a noisy image at a sampled timestep is computed by linearly interpolating between a data image and randomly sampled Gaussian noise, and the training loss is the MSE loss between the model’s prediction and \(\epsilon - \mathbf{x}\)</li> <li>Sampled timesteps are upscaled by a pre-defined factor before being input to the model</li> <li>Classifier-free guidance is employed with a condition drop rate of 0.1</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">FMLitModule</span><span class="p">(</span><span class="n">L</span><span class="p">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="nc">ConditionalUnet</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">time_scale</span> <span class="o">=</span> <span class="mi">1000</span>
        <span class="n">self</span><span class="p">.</span><span class="n">eps</span> <span class="o">=</span> <span class="mf">0.006</span>
        <span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">=</span> <span class="mf">0.994</span>

    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">batch</span>

        <span class="c1"># Randomly sample Gaussian noises
</span>        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="c1"># Randomly sample timesteps ranging from eps to T
</span>        <span class="n">sigmas</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">eps</span>
        <span class="n">sigmas</span> <span class="o">=</span> <span class="n">sigmas</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

        <span class="c1"># get noisy images at the sampled timesteps by linear interpolation
</span>        <span class="n">noisy_x</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">sigmas</span><span class="p">)</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sigmas</span> <span class="o">*</span> <span class="n">noise</span>

        <span class="c1"># Condition the model on timesteps upscaled by 1000
</span>        <span class="n">condition_timesteps</span> <span class="o">=</span> <span class="n">sigmas</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">time_scale</span>

        <span class="c1"># Let the model predict the velocities. CFG drop rate is set to be 0.1
</span>        <span class="n">pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">noisy_x</span><span class="p">,</span> <span class="n">condition_timesteps</span><span class="p">.</span><span class="nf">flatten</span><span class="p">(),</span> <span class="n">labels</span><span class="p">,</span> <span class="n">is_drop</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">rand</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">&lt;=</span> <span class="mf">0.1</span><span class="p">)</span>

        <span class="c1"># MSE loss
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">mse_loss</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">noise</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="sh">"</span><span class="s">train_loss</span><span class="sh">"</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer</span>
</pre></td></tr></tbody></table></code></pre></div></div> <p>We train the model on MNIST dataset for 10 epochs, with a batch size of 128 and a learning rate of 1e-3.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MNISTData</span><span class="p">(</span><span class="n">L</span><span class="p">.</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transform</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Compose</span><span class="p">([</span>
            <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">ToTensor</span><span class="p">(),</span>
            <span class="n">torchvision</span><span class="p">.</span><span class="n">transforms</span><span class="p">.</span><span class="nc">Normalize</span><span class="p">([</span><span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">])</span>
        <span class="p">])</span>

    <span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mnist</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="nc">MNIST</span><span class="p">(</span>
            <span class="n">root</span><span class="o">=</span><span class="sh">"</span><span class="s">../</span><span class="sh">"</span><span class="p">,</span>
            <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
            <span class="n">transform</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">transform</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">mnist</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">FMLitModule</span><span class="p">()</span>
<span class="n">data</span> <span class="o">=</span> <span class="nc">MNISTData</span><span class="p">()</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">L</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="n">loggers</span><span class="p">.</span><span class="nc">TensorBoardLogger</span><span class="p">(</span><span class="sh">"</span><span class="s">tb_logs</span><span class="sh">"</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="sh">"</span><span class="s">flow_matching_model</span><span class="sh">"</span><span class="p">)</span>

<span class="n">trainer</span> <span class="o">=</span> <span class="n">L</span><span class="p">.</span><span class="nc">Trainer</span><span class="p">(</span>
    <span class="n">max_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">logger</span><span class="o">=</span><span class="n">logger</span>
<span class="p">)</span>
<span class="n">trainer</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div> <div class="row"> <div class="mx-auto col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/cfm/cfm_mnist_train-480.webp 480w,/site/assets/img/cfm/cfm_mnist_train-800.webp 800w,/site/assets/img/cfm/cfm_mnist_train-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/cfm/cfm_mnist_train.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> training loss trajectory </div> <p>Given a pretrained model that predicts the velocity vector field at a given timestep, we can generate new samples by applying the Euler method to move from $x_0\sim \mathcal{N}(0,\mathbf{I})$ to a sample $x_1$ from MNIST distribution, as follows.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
</pre></td><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">torchvision.transforms.functional</span> <span class="kn">import</span> <span class="n">to_pil_image</span>

<span class="n">device</span> <span class="o">=</span> <span class="sh">'</span><span class="s">mps</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cuda</span><span class="sh">'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span>

<span class="n">litmodule</span> <span class="o">=</span> <span class="nc">FMLitModule</span><span class="p">()</span>
<span class="n">ckpt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span>
    <span class="sh">'</span><span class="s">/path/to/checkpoint</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">,</span>
    <span class="n">weights_only</span><span class="o">=</span><span class="bp">True</span>
<span class="p">)</span>
<span class="n">litmodule</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span>
    <span class="n">ckpt</span><span class="p">[</span><span class="sh">'</span><span class="s">state_dict</span><span class="sh">'</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">litmodule</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
<span class="n">litmodule</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
<span class="n">num_inference_steps</span> <span class="o">=</span> <span class="mi">15</span>
<span class="n">cfg_scale</span> <span class="o">=</span> <span class="mf">3.0</span>

<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">inference_mode</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">8</span><span class="o">*</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="mi">8</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]).</span><span class="nf">flatten</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">is_drop</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]).</span><span class="nf">bool</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">t_steps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="n">litmodule</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">num_inference_steps</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">t_cur</span><span class="p">,</span> <span class="n">t_next</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">tqdm</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">t_steps</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">t_steps</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))):</span>

        <span class="n">x_</span> <span class="o">=</span> <span class="n">eio</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">'</span><span class="s">b c h w -&gt; (2 b) c h w</span><span class="sh">'</span><span class="p">)</span>
        <span class="n">y_</span> <span class="o">=</span> <span class="n">eio</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="sh">'</span><span class="s">b -&gt; (2 b)</span><span class="sh">'</span><span class="p">)</span>

        <span class="n">pred</span> <span class="o">=</span> <span class="n">litmodule</span><span class="p">.</span><span class="nf">model</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">t_cur</span> <span class="o">*</span> <span class="n">litmodule</span><span class="p">.</span><span class="n">time_scale</span><span class="p">,</span> <span class="n">y_</span><span class="p">,</span> <span class="n">is_drop</span><span class="p">)</span>

        <span class="n">pred_pos</span><span class="p">,</span> <span class="n">pred_neg</span> <span class="o">=</span> <span class="n">pred</span><span class="p">.</span><span class="nf">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">cfg_scale</span> <span class="o">*</span> <span class="n">pred_pos</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">cfg_scale</span><span class="p">)</span> <span class="o">*</span> <span class="n">pred_neg</span>

        <span class="n">dt</span> <span class="o">=</span> <span class="n">t_next</span> <span class="o">-</span> <span class="n">t_cur</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">dt</span> <span class="o">*</span> <span class="n">pred</span>

<span class="n">img_grid</span> <span class="o">=</span> <span class="n">eio</span><span class="p">.</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">'</span><span class="s">(m n) 1 h w -&gt; (m h) (n w)</span><span class="sh">'</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mi">10</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">clip</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nf">to_pil_image</span><span class="p">(</span><span class="n">img_grid</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span>
</pre></td></tr></tbody></table></code></pre></div></div> <div class="row"> <div class="mx-auto col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/cfm/cfm_mnist_sample-480.webp 480w,/site/assets/img/cfm/cfm_mnist_sample-800.webp 800w,/site/assets/img/cfm/cfm_mnist_sample-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/cfm/cfm_mnist_sample.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> samples generated by the trained model using 15 inference steps and CFG scale of 3.0 </div> <p>For the complete training and inference code, please refer to <a href="https://colab.research.google.com/drive/1ieHUpYbizrBWP4ilUf11d5wcWBdoCufi?usp=sharing">this Jupyter notebook</a></p>]]></content><author><name></name></author><category term="implementation"/><category term="flow-matching-model"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Flow matching generative model</title><link href="https://mtuanbui.github.io/site/blog/2025/cfm/" rel="alternate" type="text/html" title="Flow matching generative model"/><published>2025-01-23T00:00:00+00:00</published><updated>2025-01-23T00:00:00+00:00</updated><id>https://mtuanbui.github.io/site/blog/2025/cfm</id><content type="html" xml:base="https://mtuanbui.github.io/site/blog/2025/cfm/"><![CDATA[ <p>In this post, we explore the theory behind flow-matching generative models and their connection to diffusion models. For more details, refer to <d-cite key="lipman2023"></d-cite>.</p> <p>We begin with $q_0$, a simple distribution that is easy to sample from, such as the standard normal distribution $\mathcal{N}(x;0,\mathbf{I})$. On the other hand, $q_1$ represents an unknown distribution for which we have a set of data samples. In the context of image generative models, \(q_1\) corresponds to the distribution of the image dataset. A generative model defines a mapping $f$ that transforms $q_0$ into $q_1$. In other words, if $x_0 \sim q_0$, then $f(x_0) \sim q_1$.</p> <p>This mapping $f$ can be expressed as an ordinary differential equation (ODE):</p> \[dx=u(t,x)dt\] <p>Here, $t \in [0,1]$, and $u(t,x)$ is a <em>time-dependent vector field</em> $u:[0,1] \times \mathbb{R}^d \to \mathbb{R}^d$.</p> <p>Once the ODE of the target mapping $f$ is learnt, we can sample $q_1$ by solving the ODE with a randomly sampled initial state $x_0 \sim q_0$ and $t$ progresses from 0 to 1. This process moves from $x_0$ to a sample $x_1$ from $q_1$, following the vector field $u(t,x)$, which defines the velocity at each time step $t$. The below image illustrates this process.</p> <div class="row"> <div class="mx-auto mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/cfm/flow-480.webp 480w,/site/assets/img/cfm/flow-800.webp 800w,/site/assets/img/cfm/flow-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/cfm/flow.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The solution of the ODE given an initial state $x_0$ is denoted by a time-dependent function, called <em>flow</em>, $\phi(t,x_0): [0,1] \times \mathbb{R}^d \to \mathbb{R}^d$. We can interpret flow $\phi(t,x_0)$ as the location of point $x_0$ after moving along the vector field $u$ from time $0$ to time $t$. Because initial state $x_0$ is a random variable, $x=\phi(t,x_0)$ is a random variable of some distribution $p_t(x)$. This means the vector field $u(t,x)$ generates a time-dependent probability function $p_t(x): [0,1] \times \mathbb{R}^d \to \mathbb{R}_{&gt;0}$ that satisfies:</p> \[\begin{aligned} p_0(x)&amp;=q_0(x) \\ p_1(x)&amp;=q_1(x) \end{aligned}\] <p>This time-dependent probability function is equivalent to $p_t$ in diffusion models, which is also known as <em>probability path</em>.</p> <h2 id="flow-matching-objective">Flow Matching objective</h2> <p>Consider a target probability path $p(t,x)$ and its associated vector field $u(t,x)$, Flow Matching (FM) objective is defined as follows.</p> \[\mathcal{L}_{FM}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1],x \sim p_t(x)}||v(t,x)-u(t,x)||^2 \tag{1}\] <p>where $\theta$ is the learnable parameters of a neural networks that predicts the vector field $v(t,x)$. Minimizing this objective allows us to train a neural network $v_\theta(t,x)$ to approximate the target vector field $u(t,x)$.</p> <p>Although straightforward, this objective is impractical because computing the target distribution $p(t,x)$ and vector field $u(t,x)$ is intractable.</p> <h2 id="conditional-flow-matching-objective">Conditional Flow Matching objective</h2> <p>Fortunately, the intractable Flow Matching objective can be replaced by a simpler objective, named <em>Conditional Flow Matching</em> (CFM) objective,</p> \[\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), x \sim p_t(x|x_1)}||v(t,x)-u(t,x|x_1)||^2\] <p>where $x_1$ is randomly sampled from data distribution $q_1(x)$, and $u(t,x \vert x_1)$ is the time-dependent vector field associated with the <em>conditional probability path</em> $p_t(x \vert x_1)$.</p> <p>It has been shown that \(\mathcal{L}_{FM}\) and \(\mathcal{L}_{CFM}\) have identical gradients w.r.t $\theta$, meaning they share the same optima. Furthermore, unlike \(\mathcal{L}_{FM}\), \(\mathcal{L}_{CFM}\) is computationally tractable, as we can efficiently sample from $p_t(x \vert x_1)$ and compute $u_t(t,x \vert x_1)$, provided they are appropriately constructed.</p> <h3 id="construction-of-target-p_tx-vert-x_1">Construction of target $p_t(x \vert x_1)$</h3> <p>As the Conditional Flow Matching objective works with any choice of conditional probability path $p_t(x \vert x_1)$, we can choose a Gaussian conditional probability path defined as:</p> \[p_t(x|x_1) = \mathcal{N}(x; \mu_t(x_1), \sigma_t(x_1)^2\mathbf{I}),\] <p>where $\mu_t,\sigma_t$ are the time-dependent mean and standard deviation of the time-dependent Gaussian distribution, respectively. We also set $\mu_0(x_1)=0$ and $\sigma_0(x_1)=1$ for all $x_1$, implying $p_0(x \vert x_1)=\mathcal{N}(x; 0,\mathbf{I})$. As a result of that,</p> \[p_0(x)=\int p_0(x|x_1)q_1(x_1)dx_1=\mathcal{N}(x;0,\mathbf{I}).\] <p>We set $\mu_1(x_1)=x_1$ and $\sigma_1(x_1)=\sigma_\text{min}$, where $\sigma_\text{min}$ is sufficiently small to ensure that $p_1(x \vert x_1)$ becomes a sharply concentrated Gaussian distribution centered at $x_1$. As a result, we obtain:</p> \[p_1(x)=\int p_1(x|x1)q_1(x_1)dx \approx q_1(x).\] <p>By minimizing the $\mathcal{L}_{CFM}$ objective under this choice of $p_t(x \vert x_1)$, we derive a vector field $v(t,x)$ that generates a probability path $p_t(x)$, which approximates the unknown data distribution $q_1(x)$ at $t=1$</p> <h3 id="construction-of-target-u_tx-vert-x_1">Construction of target $u_t(x \vert x_1)$</h3> <p>In addition to the target conditional probability path \(p_t(x \vert x_1)\), the \(\mathcal{L}_{CFM}\) objective also requires a time-dependent conditional vector field \(u(t,x \vert x_1)\) that generates \(p_t(x \vert x_1)\). While there are infinite such vector fields, we can choose the simplest one corresponding to the following conditional <em>flow</em>:</p> \[\phi(t,x|x_1)= \mu_t(x_1) + \sigma_t(x_1)x\] <p>where $x \sim \mathcal{N}(0,\mathbf{I})$. It is straightforward to verify that this conditional flow generates the target conditional probability path $p_t(x \vert x_1)=\mathcal{N}(x; \mu_t(x_1), \sigma_t(x_1)^2\mathbf{I})$.</p> <p>As shown in <d-cite key="lipman2023"></d-cite>, the $\phi(t,x \vert x_1)$ uniquely determines a conditional vector field, which takes the form:</p> \[u(t,x|x_1) = \frac{\sigma_t'(x_1)}{\sigma_t(x_1)}(x-\mu_t(x_1)) + \mu_t'(x_1) \tag{2}\] <h3 id="choice-of-mu_t-and-sigma_t">Choice of $\mu_t$ and $\sigma_t$</h3> <p>Depending on how we define $\mu_t$ and $\sigma_t$, we will have different Gaussian conditional probability paths. Notably, diffusion processes, such as Variance Exploding (VE) or Variance Preserving (VP), are characterized by Gaussian conditional probability paths and therefore can be interpreted as specific choices of $\mu_t$ and $\sigma_t$ within the Conditional Flow Matching framework.</p> <p>As demonstrated in <d-cite key="lipman2023"></d-cite>, the optimal choice of $\mu_t$ and $\sigma_t$ is the one that makes the conditional flow $\phi(t,x \vert x_1)$ a linear interpolation between $x$ and $x_1$.Specifically, they set</p> \[\begin{aligned} \mu_t(x_1) &amp;= tx_1 \\ \sigma_t(x_1) &amp;= 1 - t \\ \end{aligned}\] <p>which leads to</p> \[\phi(t,x|x_1)= tx_1 + (1-t)x\] <p>This special flow is called <em>Optimal Transport flow</em>, and using Equation 2, its corresponding conditional vector field is</p> \[u(t,x|x_1)=\frac{x_1 - x}{1 - t}\] <p>Substituting this into Equation 1, we get:</p> \[\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), x \sim p_t(x|x_1)}\left\|v(t,x)-\frac{x_1 - x}{1 - t}\right\|^2\] <p>while this is already a straightforward loss function that can be implemented and optimized using gradient descent, we can refine it a little more. As $\mu_t(x_1)=tx_1$ and $\sigma_t(x_1)=1 - t$, we have:</p> \[p_t(x|x_1)=\mathcal{N}(x;tx_1,(1-t)^2\mathbf{I})\] <p>which means $x \sim p_t(x \vert x_1)$ is equivalent to $x = tx_1 + (1-t)\epsilon$ where $\epsilon \sim \mathcal{N}(0,\mathbf{I})$. As a result, we can rewrite $\mathcal{L}_{CFM}$ by replacing $x \sim p_t(x \vert x_1)$ with $\epsilon \sim \mathcal{N}(0,\mathbf{I})$ and substituting $tx_1 + (1-t)\epsilon$ for $x$. Specifically,</p> \[\begin{aligned} \mathcal{L}_{CFM}(\theta) &amp;= \mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), \epsilon \sim \mathcal{N}(0,\mathbf{I})} \left\| v(t,tx_1+(1-t)\epsilon)-\frac{x_1 - (tx_1+(1-t)\epsilon)}{1 - t} \right\|^2 \\ &amp;= \mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), \epsilon \sim \mathcal{N}(0,\mathbf{I})} \left\| v(t,tx_1+(1-t)\epsilon)-(x_1-\epsilon) \right\|^2 \end{aligned}\] <p>which is the final loss function to be implemented, and it is the same as the loss function used by Flow Matching models like Stable Diffusion 3 or Flux.</p> <h2 id="diffusion-models-are-flow-matching-models">Diffusion models are Flow Matching models</h2> <p>As we know, the forward process in a diffusion model gradually degrades an observed data point, such as an image, over time by blending it with Gaussian noise. That process is defined by the following formula.</p> \[z_t=\alpha_t \mathbf{x} + \sigma_t \epsilon\] <p>where $t\in[0,1]$, $\mathbf{x}$ is an observed data point, $\epsilon \sim \mathcal{N}(0,\mathbf{I})$, and $\alpha_t$ and $\sigma_t$ are designed such that $\alpha_0 \approx 1, \sigma_0 \approx 0$ and $\alpha_1 \approx 0, \sigma_1 \approx 1$. These time-dependent parameters, $\alpha_t$ and $\sigma_t$, are commonly referred to as the <em>Noise Scheduler</em>.</p> <p>The formula can be interpreted as $z_t$ being a random sample from the Gaussian distribution $\mathcal{N}(z; \alpha_t \mathbf{x}, \sigma_t^2)$, and thereby a diffusion forward process can be characterized by conditional probability path:</p> \[p_t(z|\mathbf{x})=\mathcal{N}(z; \alpha_t \mathbf{x}, \sigma_t^2 \mathbf{I}).\] <p>Recall that the aforementioned Conditional Flow Matching objective constructs a model that are also characterized by a Gaussian conditional probability path of the form:</p> \[p_t(x|x_1) = \mathcal{N}(x; \mu_t(x_1), \psi_t(x_1)^2\mathbf{I}),\] <p>(note that I replaced $\sigma_t$ by $\psi_t$ to avoid confusion)</p> <p>Furthermore, by setting $\mu_t(x_1)=\alpha_{1-t} x_1$ and $\psi_t(x_1)=\sigma_{1-t}$, we obtain the same conditional probability path as in diffusion models. Thus, we can conclude that diffusion models are instances of Flow Matching models.</p> <p>Conversely, we can also represent the Optimal Transport flow model using a diffusion model. In this case, the noise scheduler is defined as:</p> \[\begin{aligned} \alpha_t &amp;= 1-t \\ \sigma_t &amp;= t \end{aligned}\] <p>If we train this diffusion model with a $\epsilon$-prediction denoiser, then the training objective becomes</p> \[\mathcal{L} = \mathbb{E}_{t \sim \mathcal{U}[0,1], \epsilon \sim \mathcal{N}(0,\mathbf{I})} \left( \frac{t}{1-t}\right)^2 ||\hat{\epsilon}-\epsilon||^2,\] <p>which is derived by rewriting $\mathcal{L}_{CFM}$ as mean squared error on $\epsilon$ with a specific weighting.</p>]]></content><author><name></name></author><category term="note"/><category term="diffusion-model"/><category term="flow-matching-model"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Elucidating the design space of diffusion-based generative models (a.k.a EDM model)</title><link href="https://mtuanbui.github.io/site/blog/2024/edm/" rel="alternate" type="text/html" title="Elucidating the design space of diffusion-based generative models (a.k.a EDM model)"/><published>2024-10-10T00:00:00+00:00</published><updated>2024-10-10T00:00:00+00:00</updated><id>https://mtuanbui.github.io/site/blog/2024/edm</id><content type="html" xml:base="https://mtuanbui.github.io/site/blog/2024/edm/"><![CDATA[<p>The EDM paper was introduced by Karras et al. <d-cite key="karras2022"></d-cite>. This excellent papers received Outstanding Papers Reward in NeurIPS 2022 <a href="https://arxiv.org/abs/2206.00364">https://arxiv.org/abs/2206.00364</a></p> <h2 id="contributions">Contributions</h2> <p>As summarized in <d-footnote>https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models/</d-footnote>, the main contributions of the paper are:</p> <ol> <li>A <strong>common framework</strong> for diffusion models</li> <li>Design choices related to <strong>sampling</strong> (generating images when you already have a trained denoiser)</li> <li>Design choices when <strong>training</strong> that denoiser</li> </ol> <h2 id="paper-contents">Paper contents</h2> <h3 id="1-expressing-diffusion-models-in-a-common-framework">1. Expressing diffusion models in a common framework</h3> <p>The Diffusion ODE can be expressed that:</p> \[\mathrm{d}x=[f(t)x-\frac{1}{2}g(t)^2\nabla_x\log p_t(x)]\mathrm{d}t\] <p>The issue is $f(t), g(t)$ and $p_t(x)$ not being quite useful when we like to design “working” variants of diffusion model because change in a component would lead to changes in the others in order to ensure the model converge to the data in limit. Therefore, the ODE need to be re-written to eliminate implicit dependencies between the components</p> <p><strong>a. Replace $p_t(x)$ by $p(x;\sigma)$</strong></p> <p>The perturbation kernels of Diffusion SDEs can be generalized as</p> \[\begin{aligned} x_t&amp;=s(t)\left( x_0 + \sigma(t)\epsilon\right) \textrm{ where } \epsilon \sim \mathcal{N}(0,\mathbf{I}) \end{aligned}\] <p>For example, SMLD SDE in <d-cite key="song2019"></d-cite> has</p> \[\begin{aligned} s(t)&amp;=1 \\ x(t)&amp;=x_0+\sigma(t) \epsilon \end{aligned}\] <p>And for DDPM SDE in <d-cite key="ho2020"></d-cite>,</p> \[\begin{aligned} s(t)&amp;=\frac{1}{\sqrt{\sigma(t)^2+1}} \\ x(t)&amp;=\frac{1}{\sqrt{\sigma(t)^2+1}}(x_0+\sigma(t) \epsilon) \end{aligned}\] <p>Equation 15 ~ 20 in <d-cite key="karras2022"></d-cite> proves that the marginal distribution $p_t(x)$ can be expressed as:</p> \[\begin{aligned} p_t(x)&amp;=\int p_{t}({x}\mid x_0)\mathrm{~}p_{\mathrm{data}}(x_0)\mathrm{~d}{x} \\ &amp;=s(t)^{-d}p\left( \frac{x_t}{s(t)};\sigma(t)\right) \end{aligned}\] <p>where $d$ is data dimension and $p(x;\sigma)$ is a distribution obtained by adding $\sigma$-standard deviation Gaussian noise to the original signal. Substitute $p_t(x)$ into the ODE equation, we get</p> \[\begin{aligned} \mathrm{d}x &amp;= [f(t)x-\frac{1}{2}g(t)^2\nabla_x\log p_t(x)]\mathrm{d}t \\ &amp;= \left[f(t){x}-\frac12g(t)^2 \nabla_x\log p(\frac{x}{s(t)};\sigma(t)) \right]\mathrm{~d}t \end{aligned} \tag{1}\] <p>(Refer to equation 21~24 in <d-cite key="karras2022"></d-cite> for the proof)</p> <p>$p(x;\sigma)$ is more favorable than $p_t(x)$ because it does not depend on $t$ and thus, the score $\nabla_x\log p(x;\sigma)$ does not depend on $\sigma(t),s(t)$ and how $t$ is discritized</p> <p>The score $\nabla_x\log p(x;\sigma)$ can be estimated by <em>Denoising Score Matching</em> <d-cite key="song2019"></d-cite>.Specifically, if $D_\theta(x;\sigma)$ is a neural network trained to minimize</p> \[\mathbb{E}_{x_0\sim p_\mathrm{data}}\mathbb{E}_{\sigma \sim p_\sigma}\mathbb{E}_{\epsilon\sim\mathcal{N}(0,\mathbf{I})}\|D_\theta(x_0 +\sigma\epsilon;\sigma)-{x_0}\|_2^2,\] <p>then</p> \[\nabla_{x}\log p(x;\sigma)=\frac{D_\theta({x};\sigma)-x}{\sigma^2}\] <p><strong>b. Replace $f(t),g(t)$ by $\sigma(t),s(t)$</strong></p> <p>Substitute $f(t)=\frac{\dot{s}(t)} {s(t)}$ and $g(t)=s(t)\sqrt{2\dot{\sigma}(t)\sigma(t)}$ (equation 28 and 34 in <d-cite key="karras2022"></d-cite>) into Equation 1, we get</p> \[\mathrm{d}x=\left[ \frac{\dot{s}(t)}{s(t)}x-s(t)^2\dot{\sigma}(t)\sigma(t) \nabla_x\log p(\frac{x}{s(t)};\sigma(t)) \right]\mathrm{d}t\] <p>By this equation, we can design new diffusion process by choosing $s(t)$ and $\sigma(t)$. For example, we can bring about an unusual process (but working) like in the below image.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/strange-ode-480.webp 480w,/site/assets/img/edm/strange-ode-800.webp 800w,/site/assets/img/edm/strange-ode-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/strange-ode.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a> </div> <h3 id="2-improvements-to-deterministic-sampling">2. Improvements to deterministic sampling</h3> <h4 id="improvement-1---higher-order-integrators">Improvement 1 - Higher-order integrators</h4> <ul> <li>$1^{st}$ order ODE solvers like Euler’s method lead to high error if step size is large</li> <li>High order Runge-Kutta methods require multiple evaluations of $D_\theta$ per step</li> <li>The authors claim that numerically solve the ODE using Heun’s 2nd order method provide an excellent tradeoff between error and NFE (neural function evaluations - how many times $D_\theta$ is evaluated)</li> </ul> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/heunsolver-480.webp 480w,/site/assets/img/edm/heunsolver-800.webp 800w,/site/assets/img/edm/heunsolver-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/heunsolver.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> image source: <a href="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTHHk4rFJw5yWFdsojXX_RTf-WYst939A0kjQ&amp;s">math.libretexts.org</a> </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/alg1-480.webp 480w,/site/assets/img/edm/alg1-800.webp 800w,/site/assets/img/edm/alg1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/alg1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h4 id="improvement-2---discretization-of-time-step">Improvement 2 - Discretization of time step</h4> <p>Appendix D.1 of <d-cite key="karras2022"></d-cite> concludes that step size should decrease when $\sigma$ decrease. Intuitively, the closer we approach $x_0$ the slower we should move. Karras et al. <d-cite key="karras2022"></d-cite> choose the noise level $\sigma$ at each discrete step $i$ by:</p> \[\begin{aligned} \sigma_{i &lt; N} &amp;= \left(\sigma_{\max}^{\frac1\rho}+\frac i{N-1}(\sigma_{\min}^{\frac1\rho}-\sigma_{\max}^{\frac1\rho})\right)^\rho \\ \sigma_N &amp;= 0 \end{aligned}\] <p>where $N$ is the number of denoising steps. Given $\sigma_i$, the time step $t_i$ can be obtained via the inverse function of $\sigma(t)$, namely $t_i=\sigma^{-1}(\sigma_i)$</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/stepsize1-480.webp 480w,/site/assets/img/edm/stepsize1-800.webp 800w,/site/assets/img/edm/stepsize1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/stepsize1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/stepsize2-480.webp 480w,/site/assets/img/edm/stepsize2-800.webp 800w,/site/assets/img/edm/stepsize2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/stepsize2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> The EDM time step discretization has step sizes to be long at high noise levels and short at low noise levels as shown in the right figure (image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a>). <br/> The left figure is the graph of the i-dependent noise level function given $\sigma_{\min}=80,\sigma_{\max}=0.002,\rho=7$ </div> <h4 id="improvement-3---make-the-ode-trajectory-curvature-to-be-low">Improvement 3 - Make the ODE trajectory curvature to be low</h4> <p>Higher curvature trajectories leads to higher errors made by ODE numerical solver. Choice of $s(t),\sigma(t)$ can reduce or increase curvature of the flow lines as shown in the below figure.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/different-noise-schedules-flow-line-gif-1-480.webp 480w,/site/assets/img/edm/different-noise-schedules-flow-line-gif-1-800.webp 800w,/site/assets/img/edm/different-noise-schedules-flow-line-gif-1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/different-noise-schedules-flow-line-gif-1.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a> </div> <p>Karras et. al. <d-cite key="karras2022"></d-cite> argues that the best choice which would significantly reduce the curvature is:</p> \[\begin{aligned} s(t)&amp;=1 \\ \sigma(t)&amp;=t \end{aligned}\] <p>which is also DDIM’s choice. Under this choice, the trajectory looks almost linear at both large and small $\sigma$, and substantial curvature lie in only a small region in between.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/ddim_trajectory-480.webp 480w,/site/assets/img/edm/ddim_trajectory-800.webp 800w,/site/assets/img/edm/ddim_trajectory-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/ddim_trajectory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> the trajectory of DDIM/EDM ODE (image source: <a href="https://developer.nvidia.com/blog/generative-ai-research-spotlight-demystifying-diffusion-based-models">demystifying diffusion-based models</a>) </div> <details><summary>Why this specific choice of design make the trajectory largely linear?</summary> <p>When $\sigma(t)=t,s(t)=1$, the ODE is simplified as:</p> \[\begin{aligned} \mathrm{d}x&amp;=\left[ \frac{\dot{s}(t)}{s(t)}x-s(t)^2\dot{\sigma}(t)\sigma(t)\left( \frac{D_\theta({x};\sigma(t))-{x}}{\sigma(t)^2}\right)\right]\mathrm{d}t \\&amp;= \left( \frac{x - D_\theta(x;t)}{t} \right)\mathrm{d}t \end{aligned}\] <p>At any time step $t$, if we take a full single Euler step to $t=0$ (i.e. $\Delta t = -t$),</p> \[\begin{aligned} x(t+(-t))&amp;=x(t)+\left( \frac{x(t) - D_\theta(x;t)}{t} \right)(-t) \\ &amp;=D_\theta(x;t) \end{aligned}\] <p>That means the tangent of the trajectory always points towards the denoiser output. Because the denoiser output changes slowly with the noise level $\sigma$, the trajectory is largely linear</p> </details> <h3 id="3-stochastic-sampling">3. Stochastic sampling</h3> <p>Deterministic sampling with ODE offers many benefits but sample quality is often worse then that of SDE. Karras et al. <d-cite key="karras2022"></d-cite> argue that the reason is the SDE is sum of the probability flow ODE and a time-varying <em>Langevin diffusion</em> SDE. For example, as shown in Appendix B.5 of <d-cite key="karras2022"></d-cite>, SDEs of Song et. al. <d-cite key="song2020"></d-cite> can be expressed as:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/sde_eqt-480.webp 480w,/site/assets/img/edm/sde_eqt-800.webp 800w,/site/assets/img/edm/sde_eqt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/sde_eqt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>where $\beta(t)=\dot{\sigma}(t)/\sigma(t)$</p> <p>Langevin diffusion actively correct for errors made in earlier sampling steps that results in better quality samples. An visual explanation for SDE’s advantage can be found at <a href="https://youtu.be/T0Qxzf0eaio?t=1946">https://youtu.be/T0Qxzf0eaio?t=1946</a></p> <p>Karras et al. <d-cite key="karras2022"></d-cite> propose a stochastic sampler which actually is the deterministic sampler being combined with explicitly adding and removing noise.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/alg2-480.webp 480w,/site/assets/img/edm/alg2-800.webp 800w,/site/assets/img/edm/alg2-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/alg2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The below figure illustrate the difference between the denoising steps of the stochastic and deterministic sampling algorithms</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/stochastic_sampler-480.webp 480w,/site/assets/img/edm/stochastic_sampler-800.webp 800w,/site/assets/img/edm/stochastic_sampler-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/stochastic_sampler.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>In practice, values of ${ S_\mathrm{churn},S_\mathrm{tmin},S_\mathrm{tmax},S_\mathrm{noise} }$ have to be chosen carefully depending on the specific model, otherwise generated images would have issues such as loss of detail, over-saturation etc. (refer to the paper for details)</p> <h3 id="4-preconditioning-and-training">4. Preconditioning and training</h3> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/edm-precondition-480.webp 480w,/site/assets/img/edm/edm-precondition-800.webp 800w,/site/assets/img/edm/edm-precondition-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/edm-precondition.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Why we should not train $D_{\theta}$ directly?</strong> It is advisable to keep input and output signal magnitudes ﬁxed to, e.g., unit variance, but $D_\theta$’s input and output are not. Recall that $D_\theta(x;\sigma)$ predicts clean signal $x_0$ from $x=x_0+\sigma \epsilon$ where $\epsilon \sim \mathcal{N}(0,\mathbf{I})$. Let</p> \[D_\theta(x;\sigma)=x-\sigma F_\theta(.)\] <p>will make the network predict the noise scaled to unit variance (similar to well-known $\epsilon$ prediction loss type). For input to have unit variance, we can use a $\sigma$-dependent normalization function.</p> <p><strong>But $F_\theta$’s training target is always better than $D_\theta$’s?</strong> Note that any errors made by $F_\theta$ are amplified by a factor of $\sigma$ → might harm the training at large $\sigma$</p> <p>Karras et al. concluded that a $\sigma$-dependent mixing of training targets of $F_\theta$ and $D_\theta$ is needed. They re-write $D_\theta$ in the following form:</p> \[D_\theta(x;\sigma)=c_\mathrm{skip}(\sigma)x+c_\mathrm{out}(\sigma)F_\theta(c_\mathrm{in}(\sigma)x;c_\mathrm{noise}(\sigma))\] <table> <thead> <tr> <th>Function</th> <th>Purpose</th> </tr> </thead> <tbody> <tr> <td>$c_\mathrm{out}(\sigma)$</td> <td>makes the output of $F_\theta$ be scaled to some variance</td> </tr> <tr> <td>$c_\mathrm{in}(\sigma)$</td> <td>works like a $\sigma$-dependent normalization factor that scales the input of $F_\theta$ to unit variance</td> </tr> <tr> <td>$c_\mathrm{noise}(\sigma)$</td> <td>maps noise level $\sigma$ into a conditioning input for $F_\theta$. For example, in DDPM it maps noise levels to timestep indices</td> </tr> <tr> <td>$c_\mathrm{skip}(\sigma)$</td> <td>$\sigma$-dependent skip connection that allows $F_\theta$ to estimate either $x_0$ or $\epsilon$, or something in between</td> </tr> </tbody> </table> <p>:bulb: This formula is a generalization of $x_0$-prediction, $\epsilon$-prediction, $\mathrm{v}$-prediction target!</p> <h4 id="determine-suitable-choices-of-preconditioning-functions-from-first-principles">Determine suitable choices of preconditioning functions from first principles</h4> <p>The overall training loss:</p> \[\begin{aligned} \mathbb{E}_{\sigma,y,n}\left[\lambda(\sigma)|| D(y+n;\sigma)-y ||^2_2\right], \\ \mathrm{where~} \sigma \sim p_{\mathrm{train}},y \sim p_{\mathrm{data}},n \sim \mathcal{N}(0,\sigma^2 \mathbf{I}) \end{aligned}\] <p>can be expressed equivalently as:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/loss-480.webp 480w,/site/assets/img/edm/loss-800.webp 800w,/site/assets/img/edm/loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>a. Training inputs of $F_\theta$ is required to have unit variance (Equation 114 ~ 117 of <d-cite key="karras2022"></d-cite>):</p> \[\begin{aligned} &amp;\mathrm{Var}_{y,n}[c_\mathrm{in}(\sigma)(y+n)]=1 \\ &amp;\Rightarrow c_\mathrm{in}(\sigma)=\frac{1}{\sqrt{\sigma^2+\sigma^2_\mathrm{data}}} \end{aligned}\] <p>b. The effective training target $F_\mathrm{target}$ is required to have unit variance (Equation 118 ~ 123 of <d-cite key="karras2022"></d-cite>):</p> \[\begin{aligned} &amp;\mathrm{Var}_{y,n}[F_\mathrm{target}(y,n;\sigma)]=1 \\ &amp;\Rightarrow c_\mathrm{out}(\sigma)^2=(1-c_\mathrm{skip}(\sigma))^2\sigma^2_\mathrm{data}+c_\mathrm{skip}(\sigma)^2\sigma^2 \end{aligned}\] <p>c. We want errors of $F_\theta$ are amplified as little as possible, so we select $c_\mathrm{skip}(\sigma)$ to minimize $c_\mathrm{out}(\sigma)$ (Equation 124 ~ 131 of <d-cite key="karras2022"></d-cite>):</p> \[\begin{aligned} &amp;c_\mathrm{skip}(\sigma)=\mathrm{arg~min}_{c_\mathrm{skip}(\sigma)}c_\mathrm{out}(\sigma) \\ &amp;\Rightarrow c_\mathrm{skip}(\sigma)=\frac{\sigma^2_\mathrm{data}} {(\sigma^2+\sigma^2_\mathrm{data})} \end{aligned}\] <p>substitute this into the above $c_\mathrm{out}$’s equation, we get:</p> \[c_\mathrm{out}=\frac{\sigma.\sigma_\mathrm{data}}{\sqrt{\sigma^2+\sigma^2_\mathrm{data}}}\] <p>d. The effective weight $\lambda(\sigma)c_\mathrm{out}(\sigma)^2$ is required to be uniform across noise levels $\sigma$:</p> \[\begin{aligned} &amp;\lambda(\sigma)c_\mathrm{out}(\sigma)=1 \\ &amp;\Rightarrow \lambda(\sigma)=(\sigma^2+\sigma^2_\mathrm{data})/(\sigma . \sigma_{\mathrm{data}})^2 \end{aligned}\] <p>e. How to select $p_\mathrm{train}(\sigma)$, i.e., how to sample noise levels during training?</p> <p>As shown by the Figure 5a in <d-cite key="karras2022"></d-cite>, significant reduction between initial and final loss is only at intermediate noise levels. Therefore, the authors decide to target the training efforts to the relevant range as shown by the dashed red curve. Specifically, $p_\mathrm{train}(\sigma)$ is implicitly defined by a log-normal distribution:</p> \[\mathrm{log}(\sigma) \sim \mathcal{N}(P_{\min},P^2_\mathrm{std}) \\ P_{\min}=-1.2, P_\mathrm{std}=1.2\] <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/sigma-sampling-480.webp 480w,/site/assets/img/edm/sigma-sampling-800.webp 800w,/site/assets/img/edm/sigma-sampling-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/sigma-sampling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>f. Finally, $c_\mathrm{noise}(\sigma)=\frac{1}{4}\mathrm{ln}(\sigma)$ is chosen empirically</p> <h4 id="augmentation-regularization">Augmentation regularization</h4> <p>Adopt an augmentation pipeline that consists of various geometric transformations (details in Appendix F.2 of <d-cite key="karras2022"></d-cite>)</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/augmentation-480.webp 480w,/site/assets/img/edm/augmentation-800.webp 800w,/site/assets/img/edm/augmentation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/augmentation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>To prevent the augmentations from leaking to the generated images, the authors make the augmentation parameters be a conditioning input to the neural network. During inference they are set to zero to ensure only non-augmented images are generated.</p> <h2 id="applications">Applications</h2> <p>The insights introduced by this outstanding paper bring about useful applications. I can think of two straightforward and direct applications w.r.t training and sampling.</p> <p>One application is to retrain our denoiser in continuous space while adopting EDM formulation and the optimal configurations suggested in the paper.</p> <p>Another appilcation is to boost sampling performance by applying the deterministic sampling in the paper. For example, based on Algorithm 1 and the recommended $\sigma(t)=t,s(t)=1$ we can implement an effective discrete Heun sampler that works with a pretrained DDPM model as follows.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/edm/heun-ddpm-480.webp 480w,/site/assets/img/edm/heun-ddpm-800.webp 800w,/site/assets/img/edm/heun-ddpm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/edm/heun-ddpm.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> This sampling resembles <a href="https://huggingface.co/docs/diffusers/en/api/schedulers/heun">HeunDiscreteScheduler</a> of Diffusers </div>]]></content><author><name></name></author><category term="review"/><category term="neurips2022"/><category term="diffusion-model"/><category term="score-based-model"/><summary type="html"><![CDATA[notes on the outstanding EDM paper]]></summary></entry><entry><title type="html">How SwiftBrush leverages VSD to train a one-step text-to-image generative model</title><link href="https://mtuanbui.github.io/site/blog/2024/swiftbrush/" rel="alternate" type="text/html" title="How SwiftBrush leverages VSD to train a one-step text-to-image generative model"/><published>2024-08-09T00:00:00+00:00</published><updated>2024-08-09T00:00:00+00:00</updated><id>https://mtuanbui.github.io/site/blog/2024/swiftbrush</id><content type="html" xml:base="https://mtuanbui.github.io/site/blog/2024/swiftbrush/"><![CDATA[<p>This post summarizes how <a href="https://arxiv.org/pdf/2312.05239">SwiftBrush</a> leverages <a href="https://github.com/thu-ml/prolificdreamer">Variational Score Distillation</a> (VSD) to distill a pretrained diffusion model into a single-step generative model</p> <p>IMO SwiftBrush’s main contributions are:</p> <ul> <li>Single-step text-to-image model that generate images of comparable quality to Stable Diffusion models</li> <li>Don’t need training data</li> </ul> <h2 id="methodology">Methodology</h2> <h3 id="key-idea">Key idea</h3> <ul> <li><a href="https://dreamfusion3d.github.io/">DreamFusion</a> successfully applied Score Distillation Sampling (SDS) to optimize a single 3D NeRF. Inspired by the idea of DreamFusion, we can replace the NeRF and its differentiable renderer with a trainable 1-step text-to-image generator, thereby converting the text-to-3D generation training into 1-step diffusion model distillation</li> <li>Replace Score Distillation Sampling with Variational Score Distillation (VSD) to avoid issues of SDS such as over-saturation, over-smoothing and low diversity</li> </ul> <h3 id="sds-vs-vsd">SDS vs. VSD</h3> <p><u>As for SDS</u>, parameters $\theta$ is updated via:</p> \[\nabla_\phi \mathcal{L}_{SDS} = \mathbb{E}_{t,\epsilon,c} \left[ w(t)(\epsilon_\psi (x_t,t,y) - \epsilon) \frac{\partial g(\theta,c)}{\partial \theta} \right]\] <p>where</p> <p>$\epsilon \sim \mathcal{N}(0,I)$, $x_t=\alpha_t g(\theta,c) + \sigma_t \epsilon$</p> <p>$t \sim \mathcal{U}(0.02T,0.98T)$, $T$ is the maximum timesteps of the diffusion model</p> <p>$y$ is the input text and $w(t)$ is a weighting function</p> <p>$g$ is the generative model parameterized by $\theta$ and $\epsilon_\psi$ is the pretrained teacher model which freezes during training</p> <p><u>In VSD</u>, $\theta$ is updated by a “slightly” changed formula:</p> \[\nabla_\phi \mathcal{L}_{SDS} = \mathbb{E}_{t,\epsilon,c} \left[ w(t)(\epsilon_\psi (x_t,t,y) - \epsilon_\phi (x_t,t,y,c)) \frac{\partial g(\theta,c)}{\partial \theta} \right]\] <p>Most of the formula are same but the $\epsilon$ in SDS formula is replaced by an output of an additional trainable denoiser $\epsilon_\phi$. This additional denoiser is trained by:</p> \[\min_{\epsilon_\phi} \mathbb{E}_{t,c,\epsilon} || \epsilon_\phi (x_t,t,y,c) - \epsilon ||\] <p>$\epsilon_\phi$ is trained by LoRA-finetuning a pretrained diffusion model</p> <p>$\epsilon_\phi$ is trained on the images generated by $g$, i.e. its noised input image is $x_t=\alpha_t g(\theta,c) + \sigma_t \epsilon$</p> <p>During training, we finetune $\epsilon_\phi$ and optimize $\theta$ in an interleaved manner.</p> <p>The image below illustrates the motivation behind the VDS formula. In summary, VDS encourages the one-step generative model to produce high-fidelity images by minimizing the KL divergence between approximate data distributions learned by two diffusion models: one trained on a real-world dataset, and the other trained on samples generated by the one-step model. The former can be any well-trained diffusion model, such as Stable Diffusion, while the latter is trained jointly with the one-step model in an interleaved manner.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/swiftbrush/intuitive-480.webp 480w,/site/assets/img/swiftbrush/intuitive-800.webp 800w,/site/assets/img/swiftbrush/intuitive-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/swiftbrush/intuitive.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="swiftbrush-training">SwiftBrush Training</h3> <p>As shown in the below image from the paper, SwiftBrush applies VSD to distill a 1-step generative model, i.e. student network in the image, from the pretrained teacher model. The student model and its parameters are $g$ and $\theta$ while LoRA teacher is $\epsilon_\phi$ model in the aforementioned VSD update formula. The training method is straightforward and clearly outlined in Algorithm 1 of the paper.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/swiftbrush/training-480.webp 480w,/site/assets/img/swiftbrush/training-800.webp 800w,/site/assets/img/swiftbrush/training-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/swiftbrush/training.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/swiftbrush/algo-480.webp 480w,/site/assets/img/swiftbrush/algo-800.webp 800w,/site/assets/img/swiftbrush/algo-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/swiftbrush/algo.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="implementation-details">Implementation details</h2> <ul> <li>Stable Diffusion 2.1</li> <li>Student models has the same architecture as teacher models</li> <li>Student model train learning rate = 1e-6</li> <li>LoRA rank=64, learning rate = 1e-3</li> <li>Because of nature of the algorithm, no training data is needed</li> <li>Batch size = 64</li> </ul> <h2 id="swiftbrush-1-step-generation-vs-other-models">SwiftBrush 1-step generation vs. other models</h2> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/swiftbrush/results-480.webp 480w,/site/assets/img/swiftbrush/results-800.webp 800w,/site/assets/img/swiftbrush/results-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/swiftbrush/results.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="review"/><category term="cvpr2024"/><category term="diffusion-model"/><category term="one-step-generation"/><summary type="html"><![CDATA[This post summarizes how SwiftBrush leverages Variational Score Distillation (VSD) to distill a pretrained diffusion model into a single-step generative model]]></summary></entry><entry><title type="html">InstaFlow: one step is enough for high-quality diffusion-based text-to-image generation</title><link href="https://mtuanbui.github.io/site/blog/2024/instaflow/" rel="alternate" type="text/html" title="InstaFlow: one step is enough for high-quality diffusion-based text-to-image generation"/><published>2024-07-16T00:00:00+00:00</published><updated>2024-07-16T00:00:00+00:00</updated><id>https://mtuanbui.github.io/site/blog/2024/instaflow</id><content type="html" xml:base="https://mtuanbui.github.io/site/blog/2024/instaflow/"><![CDATA[<p><a href="https://arxiv.org/abs/2309.06380">InstaFlow</a> introduces a method to transform a many-step diffusion model into a high-quality one-step generative model. The core concept involves applying a reflow process to straighten the flow trajectories before distillation training.</p> <h2 id="background">Background</h2> <p>Background in Flow Matching, Rectified Flow and Reflow is helpful to fathom the idea behind InstaFlow.</p> <h3 id="flow-matching">Flow Matching</h3> <p>The score-based model shows that diffusion process can be expressed as an ordinary differential equation (ODE) which is indirectly learned via score-matching:</p> \[\mathrm{d}x = \left[ f(x,t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x) \right] \mathrm{d}t\] <p>In a nutshell Flow matching directly learn that ODE, i.e. it learns what inside the bracket</p> <p>Given $\pi_0$ as a standard Gaussian distribution and $\pi_1$ as the image data distribution. Flow matching learns to transfer $\pi_0$ to $\pi_1$ via an ODE:</p> \[\frac{\mathrm{d}X_t}{\mathrm{d}t}=\mathcal{v}(X_t,t)\] <p>such that if the initial $X_0 \sim \pi_0$ then $X_1 \sim \pi_1$. The velocity field $\mathcal{v}$ is learned by minimizing a simple mean square objective:</p> \[\min_\mathcal{v}\mathbb{E}_{(X_0,X_1) \sim \gamma} \left[ \int_0^1 ||\frac{\mathrm{d}}{\mathrm{d}t}X_t - \mathcal{v}(X_t,t)||^2 \mathrm{d}t \right] \tag{1}\] <p>where $X_t = \phi (X_0, X_1, t)$ is an interpolation function between $X_0$ and $X_1$ that is differentiable w.r.t timestep $t$. Commonly $\phi$ has the form</p> \[X_t = \phi (X_0, X_1, t)=\alpha_tX_0 + \beta_t X_1\] <p>The specific choice of $\alpha_t,\beta_t$ make different flow models.</p> <h3 id="rectified-flow">Rectified Flow</h3> <p>Rectified Flow model suggests a special choice of $\phi$</p> \[X_t = \phi (X_0, X_1, t)= (1-t)X_0+ tX_1\] <p>which entails</p> \[\frac{\mathrm{d}X_t}{\mathrm{d}t}=X_1-X_0=const\] <p>As a result, the trajectory from $X_0$ to $X_1$ is contrainsted to a straight line, which is advantageous for ODE integration as it allows for the use of larger integration steps without compromising accuracy.</p> <h3 id="reflow">Reflow</h3> <p>Training a rectified flow using Equation 1 alone would not lead to a satisfactorily straight flow model. Study in <a href="https://arxiv.org/abs/2209.03003">https://arxiv.org/abs/2209.03003</a> suggests that we can enhance the straightness by recursively applying the rectifying procedure, as shown in the image below.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/instaflow/reflow-480.webp 480w,/site/assets/img/instaflow/reflow-800.webp 800w,/site/assets/img/instaflow/reflow-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/instaflow/reflow.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Reflow procedure. The image is from <a href="https://arxiv.org/abs/2209.03003">https://arxiv.org/abs/2209.03003</a> </div> <p>In the diagram, $(X_0,X_1)$ represents sample pairs generated by the pretrained flow matching model, while $(Z^i_0, Z^i_1)$ represents samples from the $i^{th}$ iteration of rectification. Although RectFlow uses the same loss function as flow matching, it operates on fixed sample pairs generated from the previous iteration, which enables the rectification effect.</p> <h2 id="instaflow">InstaFlow</h2> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/instaflow/overview-480.webp 480w,/site/assets/img/instaflow/overview-800.webp 800w,/site/assets/img/instaflow/overview-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/instaflow/overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>InstaFlow’s main find is: Direct distillation fails, while reflow + distillation succeeds, from which they proposes a 2-steps method to train a one-step denoising model</p> <h3 id="step-1-straighten-a-pretrained-text-conditioned-model-via-text-conditioned-reflow">Step 1: straighten a pretrained text-conditioned model via text-conditioned reflow</h3> <p>Given a pretrained text-to-image models, e.g. Stable Diffusion, this step applies Reflow to improve straightness of the model as below</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/instaflow/reflow_loss-480.webp 480w,/site/assets/img/instaflow/reflow_loss-800.webp 800w,/site/assets/img/instaflow/reflow_loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/instaflow/reflow_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/instaflow/reflow_alg-480.webp 480w,/site/assets/img/instaflow/reflow_alg-800.webp 800w,/site/assets/img/instaflow/reflow_alg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/instaflow/reflow_alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Intuitively the Reflow attempts to keep the reflowed ODE coincided with the pretrained ODE while make it as straight as possible. The training of this step relies solely on synthetic data generated by simulating the pretrained ODE which makes pairs of $(X_0,X_1)$.</p> <p>Figure 6 of the paper presents a comparison between before and after reflowing the SD 1.4 model.</p> <div class="row"> <div class="mx-auto col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/instaflow/reflow_example-480.webp 480w,/site/assets/img/instaflow/reflow_example-800.webp 800w,/site/assets/img/instaflow/reflow_example-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/instaflow/reflow_example.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>It can be seen that after reflow, straightness has been significantly improved. However, there has also been a slight change in the start and end points of the trajectories. I speculate that this represents a trade-off between high straightness without intersection and consistency with the initial flow.</p> <h3 id="step-2-text-conditioned-distillation">Step 2: text-conditioned distillation</h3> <p>While improving straightness, a limited number of iterations of step 1 cannot achieve perfect linear flow, resulting in an unusable one-step generation as illustrated in Figure 7 in the paper (the middle subfigure in the bottom row). To address this, the authors propose the following distillation technique.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/instaflow/distill_loss-480.webp 480w,/site/assets/img/instaflow/distill_loss-800.webp 800w,/site/assets/img/instaflow/distill_loss-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/instaflow/distill_loss.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/instaflow/distill_alg-480.webp 480w,/site/assets/img/instaflow/distill_alg-800.webp 800w,/site/assets/img/instaflow/distill_alg-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/instaflow/distill_alg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This means learning a single Euler step mapping $X_0$ to $\mathrm{ODE}[ \mathcal{v}_k ](X_0 \vert \mathcal{T})$ by minimizing a similarity loss $\mathbb{D}$. In the paper’s experiment, the similarity loss is LPIPS loss.</p> <p>Moreover, the author emphasizes that this step 2 will not be effective without step 1. The reason is that reflow has created a better coupling compared to the original flow in terms of transport costs, which makes it easier to train the student network.</p> <h2 id="limitations">Limitations</h2> <p>InstaFlow has few limitations:</p> <ul> <li>High cost of synthesizing and storing the dataset</li> <li>Reliance on a synthetic training dataset</li> <li>Inferior image quality compared to the original model</li> </ul> <p>These limitations have been addressed in their <a href="https://arxiv.org/abs/2405.07510">subsequent work</a></p>]]></content><author><name></name></author><category term="review"/><category term="iclr2024"/><category term="diffusion-model"/><category term="one-step-generation"/><summary type="html"><![CDATA[InstaFlow introduces a method to transform a many-step diffusion model into a high-quality one-step generative model. The core concept involves applying a reflow process to straighten the flow trajectories before distillation training.]]></summary></entry><entry><title type="html">Prompt-based image editing via Delta Denoising Score</title><link href="https://mtuanbui.github.io/site/blog/2024/dds/" rel="alternate" type="text/html" title="Prompt-based image editing via Delta Denoising Score"/><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://mtuanbui.github.io/site/blog/2024/dds</id><content type="html" xml:base="https://mtuanbui.github.io/site/blog/2024/dds/"><![CDATA[<p>Score Distillation Sampling (SDS), proposed by <a href="https://arxiv.org/abs/2209.14988">DreamFusion</a>, can be used to generate NeRFs from an input prompt. Furthermore, by replacing the parameters of a NeRF with the pixels of an image, we can utilize SDS for prompt-based image editing, as illustrated in the figure below.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/sds_dds-480.webp 480w,/site/assets/img/dds/sds_dds-800.webp 800w,/site/assets/img/dds/sds_dds-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/sds_dds.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Comparison of SDS and DDS for prompt-based image editing. The image is from DDS paper. </div> <p>Looking at the image, it can be seen that the results of SDS are blurry and have low fidelity. Specifically, the details that are lost are those that are not mentioned in the input prompt. To explain this phenomenon, the authors of Denoising Delta Score (DDS) argue that the reason is due to the gradient $\nabla_\theta \mathcal{L}_\mathrm{SDS}$ consisting of two components.</p> \[\nabla_\theta \mathcal{L}_\mathrm{SDS}=\delta_{text} + \delta_{bias}\] <p>where</p> <p>$\delta_{text}$ is the “good” direction that directs the image to the closest image matching the text and $\delta_{bias}$ is the “bad” direction that causes the image to become smooth and blurry.</p> <p>Based on that observation, DDS proposes a method to achieve a more optimal update direction, namely</p> \[\nabla_\theta \mathcal{L}_\mathrm{DDS} \approx \delta_{text}\] <p>In this post, we explore how they do that.</p> <h2 id="sds-vs-dds">SDS vs. DDS</h2> <p>To illustrate the distinction between SDS and DDS, consider a prompt-based image editing scenario. Given an input image described as <code class="language-plaintext highlighter-rouge">A turtle flying a kite</code>, as depicted below, our objective is to modify the image to align with the revised prompt <code class="language-plaintext highlighter-rouge">A turtle flying a kite at sunset</code>.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/example_problem-480.webp 480w,/site/assets/img/dds/example_problem-800.webp 800w,/site/assets/img/dds/example_problem-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/example_problem.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Suppose the set of parameters to be optimized is $\theta$, for example, $\theta$ could be the values of the pixels of input images. SDS and DDS update x in different ways:</p> <h3 id="sds">SDS</h3> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/sds_opt-480.webp 480w,/site/assets/img/dds/sds_opt-800.webp 800w,/site/assets/img/dds/sds_opt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/sds_opt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> \[\nabla_\theta \mathcal{L}_\mathrm{SDS}=\mathbb{E}_{t,\epsilon} \left[ w(t)(\epsilon_\phi(\mathbf{z}_t;y,t)-\epsilon)\frac{\partial\mathbf{z}}{\partial\theta} \right]\] \[y=\unicode{x201C}\textrm{a turtle flying a kite at sunset}\unicode{x201D}\] <h3 id="dds">DDS</h3> <p>DDS requires an additional reference image for its optimization.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/dds_opt-480.webp 480w,/site/assets/img/dds/dds_opt-800.webp 800w,/site/assets/img/dds/dds_opt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/dds_opt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> \[\nabla_\theta \mathcal{L}_\mathrm{DDS}=\nabla_\theta \mathcal{L}_\mathrm{SDS}(\mathbf{z},y) - \nabla_\theta \mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y})\] \[y=\unicode{x201C}\textrm{a turtle flying a kite at sunset}\unicode{x201D}\] \[\hat{y}=\unicode{x201C}\textrm{a turtle flying a kite}\unicode{x201D}\] <h2 id="why-do-dds-images-look-better-than-sds-ones">Why do DDS images look better than SDS ones?</h2> <p>Firstly, the gradient of $\mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y})$ is equivalent to the “bad” direction. Specifically,</p> \[\nabla_\theta \mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y}) = \hat{\delta}_{bias}\] <p>which can be proved visually by applying SDS optimization on a well-matched pair of image and prompt, as shown in the image below.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/bad_direction-480.webp 480w,/site/assets/img/dds/bad_direction-800.webp 800w,/site/assets/img/dds/bad_direction-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/bad_direction.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Moreover, “bad” directions of closely related images are similar. Specifically,</p> \[\delta_{bias} \approx \hat{\delta}_{bias}\] <p>as shown by cosine similarity between SDS update directions of matched pairs.</p> <div class="row"> <div class="mx-auto col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/cosine_sim-480.webp 480w,/site/assets/img/dds/cosine_sim-800.webp 800w,/site/assets/img/dds/cosine_sim-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/cosine_sim.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>From these observations, we can conclude that</p> \[\nabla_\theta \mathcal{L}_\mathrm{DDS}=\nabla_\theta \mathcal{L}_\mathrm{SDS}(\mathbf{z},y) - \nabla_\theta \mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y}) \approx \delta_{text}\] <p>Thus, DDS update direction concentrates on editing the relevant portion of the image such that it matches the target prompt.</p> <div class="row"> <div class="mx-auto col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/dds_grad-480.webp 480w,/site/assets/img/dds/dds_grad-800.webp 800w,/site/assets/img/dds/dds_grad-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/dds_grad.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="train-image-to-image-translation-model-using-dds">Train Image-to-Image translation model using DDS</h2> <p>Editing an image by incrementally updating with DDS direction has several drawbacks:</p> <ul> <li>Require prompts for both the input and the edited image</li> <li>Inference time is long</li> </ul> <p>To address these shortcomings, the authors of DDS propose a novel unsupervised training for a task-driven image-to-image model. The training process of such a model is illustrated in the figure below.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/dds/img-to-img-480.webp 480w,/site/assets/img/dds/img-to-img-800.webp 800w,/site/assets/img/dds/img-to-img-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/dds/img-to-img.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> How to train a task-driven image-to-image model using DDS. The image is from DDS paper. </div> <p>The idea here is that instead of directly optimizing the edited images, we optimize the weights of a neural network that outputs the edited images given the input images. As shown in the figure above, the weights of this image-to-image model are optimized using the DDS update direction. Additionally, we can make the model more versatile by conditioning it on task embeddings, allowing it to edit the input image based on the given task. To enable this, we need a suitable dataset that ensures the input task and the caption of the output image are well matched.</p>]]></content><author><name></name></author><category term="review"/><category term="iccv2023"/><category term="diffusion-model"/><category term="score-distillation"/><summary type="html"><![CDATA[Score Distillation Sampling (SDS), proposed by DreamFusion, can be used to generate NeRFs from an input prompt. Furthermore, by replacing the parameters of a NeRF with the pixels of an image, we can utilize SDS for prompt-based image editing, as illustrated in the figure below.]]></summary></entry><entry><title type="html">Explore the Score Distillation Sampling technique</title><link href="https://mtuanbui.github.io/site/blog/2024/sds/" rel="alternate" type="text/html" title="Explore the Score Distillation Sampling technique"/><published>2024-06-21T00:00:00+00:00</published><updated>2024-06-21T00:00:00+00:00</updated><id>https://mtuanbui.github.io/site/blog/2024/sds</id><content type="html" xml:base="https://mtuanbui.github.io/site/blog/2024/sds/"><![CDATA[<p>In this post, I will delve into Score Distillation Sampling (SDS), the core method powering <a href="https://arxiv.org/abs/2209.14988">DreamFusion</a></p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/sds/dreamfusion-480.webp 480w,/site/assets/img/sds/dreamfusion-800.webp 800w,/site/assets/img/sds/dreamfusion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/sds/dreamfusion.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> DreamFusion training diagram. The image is from DreamFusion paper. </div> <p>DreamFusion is a text-to-3D synthesis pipeline that optimizes a Neural Radiance Field (NeRF) to align with a given text prompt. The synthesis procedure in DreamFusion is as follows:</p> <ol> <li>Randomly initialized NeRF from scratch</li> <li>Randomly sample a camera and light</li> <li>Using a differentiable render to render an image of the NeRF from that camera and shade with the light</li> <li>Compute gradient of the SDS loss w.r.t the NeRF params</li> <li>Update NeRF params and repeat from step 2</li> </ol> <p>At the core of DreamFusion lies the SDS loss, which we will explore.</p> <h2 id="my-sds-interpretation">My SDS interpretation</h2> <p>From my perspective, the SDS loss shares some similarities with the discriminator loss in GANs, as it evaluates the fidelity of an input image. However, unlike GANs where the discriminator is trained concurrently with the generator, SDS utilizes a pre-trained, frozen diffusion model as its discriminator.</p> <p>Assuming $\mathbf{x}=g_\theta (y)$ is the image generated by a generator $g$ parameterized by $\theta$ and $y$ is the input prompt, our objective is to optimize $\theta$ such that $\mathbf{x}$ looks like a sample $x_0 \sim p_0(x \vert y)$ implicitly defined by the frozen diffusion model. This objective means the diffusion loss given $x$ as the clean signal must be small. Specifically, we optimize:</p> \[\theta^*=\min_\theta \mathcal{L}_\mathrm{Diff}(\phi,\mathbf{x}=g_\theta(y))\] <p>where $\phi$ is params of the pretrained frozen diffusion model and</p> \[\mathcal{L}_\mathrm{Diff}(\phi,\mathbf{x})=\mathbb{E}_{t,\epsilon} || w(t)(\hat\epsilon_\phi(\mathbf{z}_t;y,t)-\epsilon) ||^2_2\] \[\mathbf{z}_t=\sqrt{\bar\alpha_t}\mathbf{x} + \sqrt{1-\bar\alpha_t}\epsilon\] <p>Note that here I assume we are using DDPM $\epsilon$-prediction type model.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/sds/train-480.webp 480w,/site/assets/img/sds/train-800.webp 800w,/site/assets/img/sds/train-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/sds/train.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="sds-loss">SDS Loss</h2> <p>In practice, it turn outs that the above loss doesn’t work well to produce realistic samples because computation of gradient of $\mathcal{L}_\mathrm{Diff}$ is unstable.</p> <p>If we break down gradient of $\mathcal{L}_\mathrm{Diff}$, we have</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/sds/gradient_eqt-480.webp 480w,/site/assets/img/sds/gradient_eqt-800.webp 800w,/site/assets/img/sds/gradient_eqt-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/sds/gradient_eqt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The authors of DreamFusion propose to omit the U-Net Jacobian terms because:</p> <ul> <li>It is expensive to compute</li> <li>It is poorly conditioned for small noise levels</li> </ul> <p>which results in $\theta$ being updated by the following gradient:</p> <div class="row"> <div class="mx-auto col-sm-9 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/sds/sds_formula-480.webp 480w,/site/assets/img/sds/sds_formula-800.webp 800w,/site/assets/img/sds/sds_formula-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/sds/sds_formula.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This formula is more concise and efficient since we no longer need to backpropagate through the diffusion network.</p> <h2 id="implementation">Implementation</h2> <p>There are two ways to update $\theta$ at each optimizing step:</p> <ol> <li>Directly compute $\nabla_\theta \mathcal{L}_\mathrm{SDS}$ via the above formula and use the result to update $\theta$</li> <li>Make pytorch and optimizer compute the gradient and update $\theta$. In order to do it, we have to find a $ \mathcal{L}_\mathrm{SDS} $ satisfying $\nabla_\theta \mathcal{L}_\mathrm{SDS}$, which is as simple as</li> </ol> \[\mathcal{L}_\mathrm{SDS}=\mathbb{E}_{t,\epsilon} \left[ w(t) (\hat\epsilon_\phi(\mathbf{z};y,t)-\epsilon) \mathbf{x} \right]\] <h2 id="limitations">Limitations</h2> <p>SDS has issues of mode collapse that generates non-diverse and blurry outputs that only highlight elements mentioned in the prompt, as shown in the image below.</p> <div class="row"> <div class="mx-auto col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/site/assets/img/sds/limitation-480.webp 480w,/site/assets/img/sds/limitation-800.webp 800w,/site/assets/img/sds/limitation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/site/assets/img/sds/limitation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="review"/><category term="iclr2023"/><category term="diffusion-model"/><category term="score-distillation"/><summary type="html"><![CDATA[In this post, I will delve into Score Distillation Sampling (SDS), the core method powering DreamFusion]]></summary></entry></feed>