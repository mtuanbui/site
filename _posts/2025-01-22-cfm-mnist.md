---
layout: post
title: A pytorch-lightning implementation of MNIST flow matching generative model 
description:
tags: flow-matching-model
categories: implementation
giscus_comments: false
date: 2025-04-05
featured: false
related_posts: false
---

<!--- cSpell:ignore mathbb,mathcal,mathbf  --->
In this post, we will apply the theory of the Optimal Transport Flow Matching model, as introduced in the [previous post](../cfm/), to train a class-conditioned flow matching model for generating samples from the MNIST dataset distribution. Weâ€™ll use the PyTorch Lightning framework to avoid boilerplate code and focus solely on the core model logic.

We use a UNet as the architecture for the flow matching model in this implementation. Since the main purpose is to demonstrate flow matching model training, we don't focus on the neural network design and instead re-use the built-in UNet implementation from ***Diffusers*** library. The network architecture is similar to the one shown [here](https://huggingface.co/learn/diffusion-course/en/unit2/3), where class labels are encoded via an embedding layer, and the resulting class embeddings are concatenated to the UNet input along the channel dimension. We also modify ```forward``` function to support classifier-free guidance (CFG) training and utilize ```einops``` to reshape tensors for better clarity. The network implementation is kept as simple as the following:

```python
class ConditionalUnet(nn.Module):
    def __init__(self, num_classes=10, class_emb_size=1):
        super().__init__()
        self.class_emb = nn.Embedding(num_classes, class_emb_size)
        self.model = UNet2DModel(
            sample_size=28,
            in_channels = 1 + class_emb_size,
            out_channels = 1,
            layers_per_block=2,
            block_out_channels=(32, 64, 64),
            down_block_types=(
                "DownBlock2D",
                "AttnDownBlock2D",
                "AttnDownBlock2D"
            ),
            up_block_types=(
                "AttnUpBlock2D",
                "AttnUpBlock2D",
                "UpBlock2D",
            )
        )

    def forward(self, x, t, class_labels, is_drop=None):
        bs, ch, h, w = x.shape

        # Get class embeddings
        class_cond = self.class_emb(class_labels)

        # Drop conditions for CFG training
        if is_drop is not None:
          class_cond[is_drop] = torch.zeros_like(class_cond[is_drop])

        # Concatenate class embeddings to input x
        class_cond = eio.repeat(class_cond, "bs c -> bs c h w", w=w, h=h)
        net_input = torch.cat([x, class_cond], dim=1)

        return self.model(net_input, t).sample
```

We now proceed to implement model training with ***Pytorch-lightning***. Here are some notes regarding this implementation.
- As an implementation of the ***Optimal Transport Flow*** model, a noisy image at a sampled timestep is computed by linearly interpolating between a data image and randomly sampled Gaussian noise, and the training loss is the MSE loss between the model's prediction and $$\epsilon - \mathbf{x}$$
- Sampled timesteps are upscaled by a pre-defined factor before being input to the model
- Classifier-free guidance is employed with a condition drop rate of 0.1

```python
class FMLitModule(L.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = ConditionalUnet()
        self.time_scale = 1000
        self.eps = 0.006
        self.T = 0.994

    def training_step(self, batch, batch_idx):
        x, labels = batch

        # Randomly sample Gaussian noises
        noise = torch.randn_like(x)

        # Randomly sample timesteps ranging from eps to T
        sigmas = torch.rand(x.shape[0], 1, 1, 1) * (self.T - self.eps) + self.eps
        sigmas = sigmas.to(self.device)

        # get noisy images at the sampled timesteps by linear interpolation
        noisy_x = (1.0 - sigmas) * x + sigmas * noise

        # Condition the model on timesteps upscaled by 1000
        condition_timesteps = sigmas * self.time_scale

        # Let the model predict the velocities. CFG drop rate is set to be 0.1
        pred = self.model(noisy_x, condition_timesteps.flatten(), labels, is_drop=torch.rand(x.shape[0]) <= 0.1)

        # MSE loss
        loss = F.mse_loss(pred, noise - x)

        self.log("train_loss", loss, prog_bar=True)
        return loss

    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)
        return optimizer
```

We train the model on MNIST dataset for 10 epochs, with a batch size of 128 and a learning rate of 1e-3.

```python
class MNISTData(L.LightningDataModule):
    def __init__(self):
        super().__init__()
        self.transform = torchvision.transforms.Compose([
            torchvision.transforms.ToTensor(),
            torchvision.transforms.Normalize([0.5], [0.5])
        ])

    def prepare_data(self):
        self.mnist = torchvision.datasets.MNIST(
            root="../",
            train=True,
            download=True,
            transform=self.transform
        )

    def train_dataloader(self):
        return DataLoader(self.mnist, batch_size=128, shuffle=True, num_workers=2)

model = FMLitModule()
data = MNISTData()

logger = L.pytorch.loggers.TensorBoardLogger("tb_logs", name="flow_matching_model")

trainer = L.Trainer(
    max_epochs=10,
    logger=logger
)
trainer.fit(model, data)
```

<div class="row">
  <div class="mx-auto col-sm-6 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/cfm/cfm_mnist_train.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class="caption">
  training loss trajectory
</div>

Given a pretrained model that predicts the velocity vector field at a given timestep, we can generate new samples by applying the Euler method to move from $x_0\sim \mathcal{N}(0,\mathbf{I})$ to a sample $x_1$ from MNIST distribution, as follows.

```python
from torchvision.transforms.functional import to_pil_image

device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'

litmodule = FMLitModule()
ckpt = torch.load(
    '/path/to/checkpoint',
    map_location='cpu',
    weights_only=True
)
litmodule.load_state_dict(
    ckpt['state_dict']
)
litmodule.eval()
litmodule.to(device)

torch.manual_seed(123)
num_inference_steps = 15
cfg_scale = 3.0

with torch.inference_mode():
    x = torch.randn(8*10, 1, 28, 28).to(device)
    y = torch.tensor([[i]*8 for i in range(10)]).flatten().to(device)

    is_drop = torch.cat([torch.zeros(x.shape[0]), torch.ones(x.shape[0])]).bool().to(device)

    t_steps = torch.linspace(litmodule.T, 0, steps=num_inference_steps + 1, device=device)

    for i, (t_cur, t_next) in enumerate(tqdm(zip(t_steps[:-1], t_steps[1:]))):

        x_ = eio.repeat(x, 'b c h w -> (2 b) c h w')
        y_ = eio.repeat(y, 'b -> (2 b)')

        pred = litmodule.model(x_, t_cur * litmodule.time_scale, y_, is_drop)

        pred_pos, pred_neg = pred.chunk(2, dim=0)
        pred = cfg_scale * pred_pos + (1 - cfg_scale) * pred_neg

        dt = t_next - t_cur
        x = x + dt * pred

img_grid = eio.rearrange(x, '(m n) 1 h w -> (m h) (n w)', m=10).detach().cpu().clip(-1, 1)
to_pil_image(img_grid * 0.5 + 0.5)
```

<div class="row">
  <div class="mx-auto col-sm-6 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/cfm/cfm_mnist_sample.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class="caption">
  samples generated by the trained model using 15 inference steps and CFG scale of 3.0
</div>

For the complete training and inference code, please refer to [this Jupyter notebook](https://colab.research.google.com/drive/1ieHUpYbizrBWP4ilUf11d5wcWBdoCufi?usp=sharing)