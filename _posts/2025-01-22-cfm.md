---
layout: distill
title: Flow matching generative model
description:
tags: diffusion-model flow-matching-model
categories: note
giscus_comments: false
date: 2025-01-23
featured: false
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

bibliography: 2018-12-22-distill.bib

toc:
  - name: Flow Matching objective
  - name: Conditional Flow Matching objective
  - name: Diffusion models are Flow Matching models
---

<!--- cSpell:ignore mathbb,mathcal,mathbf  --->

In this post, we explore the theory behind flow-matching generative models and their connection to diffusion models. For more details, refer to <d-cite key=lipman2023></d-cite>.

We begin with $q_0$, a simple distribution that is easy to sample from, such as the standard normal distribution $\mathcal{N}(x;0,\mathbf{I})$. On the other hand, $q_1$ represents an unknown distribution for which we have a set of data samples. In the context of image generative models, $$q_1$$ corresponds to the distribution of the image dataset. A generative model defines a mapping $f$ that transforms $q_0$ into $q_1$. In other words, if $x_0 \sim q_0$, then $f(x_0) \sim q_1$.

This mapping $f$ can be expressed as an ordinary differential equation (ODE):

$$
dx=u(t,x)dt
$$

Here, $t \in [0,1]$, and $u(t,x)$ is a _time-dependent vector field_ $u:[0,1] \times \mathbb{R}^d \to \mathbb{R}^d$.

Once the ODE of the target mapping $f$ is learnt, we can sample $q_1$ by solving the ODE with a randomly sampled initial state $x_0 \sim q_0$ and $t$ progresses from 0 to 1. This process moves from $x_0$ to a sample $x_1$ from $q_1$, following the vector field $u(t,x)$, which defines the velocity at each time step $t$. The below image illustrates this process.

<div class="row">
  <div class="mx-auto mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/cfm/flow.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

The solution of the ODE given an initial state $x_0$ is denoted by a time-dependent function, called _flow_, $\phi(t,x_0): [0,1] \times \mathbb{R}^d \to \mathbb{R}^d$. We can interpret flow $\phi(t,x_0)$ as the location of point $x_0$ after moving along the vector field $u$ from time $0$ to time $t$.
Because initial state $x_0$ is a random variable, $x=\phi(t,x_0)$ is a random variable of some distribution $p_t(x)$. This means the vector field $u(t,x)$ generates a time-dependent probability function $p_t(x): [0,1] \times \mathbb{R}^d \to \mathbb{R}_{>0}$ that satisfies:

$$
\begin{aligned}
p_0(x)&=q_0(x) \\
p_1(x)&=q_1(x)
\end{aligned}
$$

This time-dependent probability function is equivalent to $p_t$ in diffusion models, which is also known as _probability path_.

## Flow Matching objective

Consider a target probability path $p(t,x)$ and its associated vector field $u(t,x)$, Flow Matching (FM) objective is defined as follows.

$$
\mathcal{L}_{FM}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1],x \sim p_t(x)}||v(t,x)-u(t,x)||^2
\tag{1}
$$

where $\theta$ is the learnable parameters of a neural networks that predicts the vector field $v(t,x)$. Minimizing this objective allows us to train a neural network $v_\theta(t,x)$ to approximate the target vector field $u(t,x)$.

Although straightforward, this objective is impractical because computing the target distribution $p(t,x)$ and vector field $u(t,x)$ is intractable.

## Conditional Flow Matching objective

Fortunately, the intractable Flow Matching objective can be replaced by a simpler objective, named _Conditional Flow Matching_ (CFM) objective,

$$
\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), x \sim p_t(x|x_1)}||v(t,x)-u(t,x|x_1)||^2
$$

where $x_1$ is randomly sampled from data distribution $q_1(x)$, and $u(t,x \vert x_1)$ is the time-dependent vector field associated with the _conditional probability path_ $p_t(x \vert x_1)$.

It has been shown that $$\mathcal{L}_{FM}$$ and $$\mathcal{L}_{CFM}$$ have identical gradients w.r.t $\theta$, meaning they share the same optima. Furthermore, unlike $$\mathcal{L}_{FM}$$, $$\mathcal{L}_{CFM}$$ is computationally tractable, as we can efficiently sample from $p_t(x \vert x_1)$ and compute $u_t(t,x \vert x_1)$, provided they are appropriately constructed.

### Construction of target $p_t(x \vert x_1)$

As the Conditional Flow Matching objective works with any choice of conditional probability path $p_t(x \vert x_1)$, we can choose a Gaussian conditional probability path defined as:

$$
p_t(x|x_1) = \mathcal{N}(x; \mu_t(x_1), \sigma_t(x_1)^2\mathbf{I}),
$$

where $\mu_t,\sigma_t$ are the time-dependent mean and standard deviation of the time-dependent Gaussian distribution, respectively. We also set $\mu_0(x_1)=0$ and $\sigma_0(x_1)=1$ for all $x_1$, implying $p_0(x \vert x_1)=\mathcal{N}(x; 0,\mathbf{I})$. As a result of that,

$$
p_0(x)=\int p_0(x|x_1)q_1(x_1)dx_1=\mathcal{N}(x;0,\mathbf{I}).
$$

We set $\mu_1(x_1)=x_1$ and $\sigma_1(x_1)=\sigma_\text{min}$, where $\sigma_\text{min}$ is sufficiently small to ensure that $p_1(x \vert x_1)$ becomes a sharply concentrated Gaussian distribution centered at $x_1$. As a result, we obtain:

$$
p_1(x)=\int p_1(x|x1)q_1(x_1)dx \approx q_1(x).
$$

By minimizing the $\mathcal{L}_{CFM}$ objective under this choice of $p_t(x \vert x_1)$, we derive a vector field $v(t,x)$ that generates a probability path $p_t(x)$, which approximates the unknown data distribution $q_1(x)$ at $t=1$

### Construction of target $u_t(x \vert x_1)$

In addition to the target conditional probability path $$p_t(x \vert x_1)$$, the $$\mathcal{L}_{CFM}$$ objective also requires a time-dependent conditional vector field $$u(t,x \vert x_1)$$ that generates $$p_t(x \vert x_1)$$. While there are infinite such vector fields, we can choose the simplest one corresponding to the following conditional _flow_:

$$
\phi(t,x|x_1)= \mu_t(x_1) + \sigma_t(x_1)x
$$

where $x \sim \mathcal{N}(0,\mathbf{I})$. It is straightforward to verify that this conditional flow generates the target conditional probability path $p_t(x \vert x_1)=\mathcal{N}(x; \mu_t(x_1), \sigma_t(x_1)^2\mathbf{I})$.

As shown in <d-cite key=lipman2023></d-cite>, the $\phi(t,x \vert x_1)$ uniquely determines a conditional vector field, which takes the form:

$$
u(t,x|x_1) = \frac{\sigma_t'(x_1)}{\sigma_t(x_1)}(x-\mu_t(x_1)) + \mu_t'(x_1)
\tag{2}
$$

### Choice of $\mu_t$ and $\sigma_t$

Depending on how we define $\mu_t$ and $\sigma_t$, we will have different Gaussian conditional probability paths. Notably, diffusion processes, such as Variance Exploding (VE) or Variance Preserving (VP), are characterized by Gaussian conditional probability paths and therefore can be interpreted as specific choices of $\mu_t$ and $\sigma_t$ within the Conditional Flow Matching framework.

As demonstrated in <d-cite key=lipman2023></d-cite>, the optimal choice of $\mu_t$ and $\sigma_t$ is the one that makes the conditional flow $\phi(t,x \vert x_1)$ a linear interpolation between $x$ and $x_1$.Specifically, they set

$$
\begin{aligned}
\mu_t(x_1) &= tx_1 \\
\sigma_t(x_1) &= 1 - t \\
\end{aligned}
$$

which leads to

$$
\phi(t,x|x_1)= tx_1 + (1-t)x
$$

This special flow is called _Optimal Transport flow_, and using Equation 2, its corresponding conditional vector field is

$$
u(t,x|x_1)=\frac{x_1 - x}{1 - t}
$$

Substituting this into Equation 1, we get:

$$
\mathcal{L}_{CFM}(\theta)=\mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), x \sim p_t(x|x_1)}\left\|v(t,x)-\frac{x_1 - x}{1 - t}\right\|^2
$$

while this is already a straightforward loss function that can be implemented and optimized using gradient descent, we can refine it a little more. As $\mu_t(x_1)=tx_1$ and $\sigma_t(x_1)=1 - t$, we have:

$$
p_t(x|x_1)=\mathcal{N}(x;tx_1,(1-t)^2\mathbf{I})
$$

which means $x \sim p_t(x \vert x_1)$ is equivalent to $x = tx_1 + (1-t)\epsilon$ where $\epsilon \sim \mathcal{N}(0,\mathbf{I})$. As a result, we can rewrite $\mathcal{L}_{CFM}$ by replacing $x \sim p_t(x \vert x_1)$ with $\epsilon \sim \mathcal{N}(0,\mathbf{I})$ and substituting $tx_1 + (1-t)\epsilon$ for $x$. Specifically,

$$
\begin{aligned}
\mathcal{L}_{CFM}(\theta) &= \mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), \epsilon \sim \mathcal{N}(0,\mathbf{I})} \left\| v(t,tx_1+(1-t)\epsilon)-\frac{x_1 - (tx_1+(1-t)\epsilon)}{1 - t} \right\|^2 \\
&= \mathbb{E}_{t \sim \mathcal{U}[0,1],x_1 \sim q_1(x), \epsilon \sim \mathcal{N}(0,\mathbf{I})} \left\| v(t,tx_1+(1-t)\epsilon)-(x_1-\epsilon) \right\|^2
\end{aligned}
$$

which is the final loss function to be implemented, and it is the same as the loss function used by Flow Matching models like Stable Diffusion 3 or Flux.

## Diffusion models are Flow Matching models

As we know, the forward process in a diffusion model gradually degrades an observed data point, such as an image, over time by blending it with Gaussian noise. That process is defined by the following formula.

$$
z_t=\alpha_t \mathbf{x} + \sigma_t \epsilon
$$

where $t\in[0,1]$, $\mathbf{x}$ is an observed data point, $\epsilon \sim \mathcal{N}(0,\mathbf{I})$, and $\alpha_t$ and $\sigma_t$ are designed such that $\alpha_0 \approx 1, \sigma_0 \approx 0$ and $\alpha_1 \approx 0, \sigma_1 \approx 1$. These time-dependent parameters, $\alpha_t$ and $\sigma_t$, are commonly referred to as the _Noise Scheduler_.

The formula can be interpreted as $z_t$ being a random sample from the Gaussian distribution $\mathcal{N}(z; \alpha_t \mathbf{x}, \sigma_t^2)$, and thereby a diffusion forward process can be characterized by conditional probability path:

$$
p_t(z|\mathbf{x})=\mathcal{N}(z; \alpha_t \mathbf{x}, \sigma_t^2 \mathbf{I}).
$$

Recall that the aforementioned Conditional Flow Matching objective constructs a model that are also characterized by a Gaussian conditional probability path of the form:

$$
p_t(x|x_1) = \mathcal{N}(x; \mu_t(x_1), \psi_t(x_1)^2\mathbf{I}),
$$

(note that I replaced $\sigma_t$ by $\psi_t$ to avoid confusion)

Furthermore, by setting $\mu_t(x_1)=\alpha_{1-t} x_1$ and $\psi_t(x_1)=\sigma_{1-t}$, we obtain the same conditional probability path as in diffusion models. Thus, we can conclude that diffusion models are instances of Flow Matching models.

Conversely, we can also represent the Optimal Transport flow model using a diffusion model. In this case, the noise scheduler is defined as:

$$
\begin{aligned}
\alpha_t &= 1-t \\
\sigma_t &= t
\end{aligned}
$$

If we train this diffusion model with a $\epsilon$-prediction denoiser, then the training objective becomes

$$
\mathcal{L} = \mathbb{E}_{t \sim \mathcal{U}[0,1], \epsilon \sim \mathcal{N}(0,\mathbf{I})} \left( \frac{t}{1-t}\right)^2 ||\hat{\epsilon}-\epsilon||^2,
$$

which is derived by rewriting $\mathcal{L}_{CFM}$ as mean squared error on $\epsilon$ with a specific weighting.
