---
layout: distill
title: How SwiftBrush leverages VSD to train a one-step text-to-image generative model
description:
tags: cvpr2024 diffusion-model one-step-generation
categories: review
giscus_comments: false
date: 2024-08-09
featured: false
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

bibliography: 2018-12-22-distill.bib
---

This post summarizes how [SwiftBrush](https://arxiv.org/pdf/2312.05239) leverages [Variational Score Distillation](https://github.com/thu-ml/prolificdreamer) (VSD) to distill a pretrained diffusion model into a single-step generative model

IMO SwiftBrush's main contributions are:

- Single-step text-to-image model that generate images of comparable quality to Stable Diffusion models
- Don't need training data

## Methodology

### Key idea

- [DreamFusion](https://dreamfusion3d.github.io/) successfully applied Score Distillation Sampling (SDS) to optimize a single 3D NeRF. Inspired by the idea of DreamFusion, we can replace the NeRF and its differentiable renderer with a trainable 1-step text-to-image generator, thereby converting the text-to-3D generation training into 1-step diffusion model distillation
- Replace Score Distillation Sampling with Variational Score Distillation (VSD) to avoid issues of SDS such as over-saturation, over-smoothing and low diversity

### SDS vs. VSD

<u>As for SDS</u>, parameters $\theta$ is updated via:

$$
\nabla_\phi \mathcal{L}_{SDS} = \mathbb{E}_{t,\epsilon,c} \left[ w(t)(\epsilon_\psi (x_t,t,y) - \epsilon) \frac{\partial g(\theta,c)}{\partial \theta}  \right]
$$

where

$\epsilon \sim \mathcal{N}(0,I)$, $x_t=\alpha_t g(\theta,c) + \sigma_t \epsilon$

$t \sim \mathcal{U}(0.02T,0.98T)$, $T$ is the maximum timesteps of the diffusion model

$y$ is the input text and $w(t)$ is a weighting function

$g$ is the generative model parameterized by $\theta$ and $\epsilon_\psi$ is the pretrained teacher model which freezes during training

<u>In VSD</u>, $\theta$ is updated by a "slightly" changed formula:

$$
\nabla_\phi \mathcal{L}_{SDS} = \mathbb{E}_{t,\epsilon,c} \left[ w(t)(\epsilon_\psi (x_t,t,y) - \epsilon_\phi (x_t,t,y,c)) \frac{\partial g(\theta,c)}{\partial \theta}  \right]
$$

Most of the formula are same but the $\epsilon$ in SDS formula is replaced by an output of an additional trainable denoiser $\epsilon_\phi$. This additional denoiser is trained by:

$$
\min_{\epsilon_\phi} \mathbb{E}_{t,c,\epsilon} || \epsilon_\phi (x_t,t,y,c) - \epsilon ||
$$

$\epsilon_\phi$ is trained by LoRA-finetuning a pretrained diffusion model

$\epsilon_\phi$ is trained on the images generated by $g$, i.e. its noised input image is $x_t=\alpha_t g(\theta,c) + \sigma_t \epsilon$

During training, we finetune $\epsilon_\phi$ and optimize $\theta$ in an interleaved manner.

The image below illustrates the motivation behind the VDS formula. In summary, VDS encourages the one-step generative model to produce high-fidelity images by minimizing the KL divergence between approximate data distributions learned by two diffusion models: one trained on a real-world dataset, and the other trained on samples generated by the one-step model. The former can be any well-trained diffusion model, such as Stable Diffusion, while the latter is trained jointly with the one-step model in an interleaved manner.

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/swiftbrush/intuitive.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

### SwiftBrush Training

As shown in the below image from the paper, SwiftBrush applies VSD to distill a 1-step generative model, i.e. student network in the image, from the pretrained teacher model. The student model and its parameters are $g$ and $\theta$ while LoRA teacher is $\epsilon_\phi$ model in the aforementioned VSD update formula. The training method is straightforward and clearly outlined in Algorithm 1 of the paper.

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/swiftbrush/training.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

<div class="row">
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
  <div class="mx-auto col-sm-8 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/swiftbrush/algo.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
  <!-- <div class="col-sm mt-3 mt-md-0"></div> -->
</div>

## Implementation details

- Stable Diffusion 2.1
- Student models has the same architecture as teacher models
- Student model train learning rate = 1e-6
- LoRA rank=64, learning rate = 1e-3
- Because of nature of the algorithm, no training data is needed
- Batch size = 64

## SwiftBrush 1-step generation vs. other models

<div class="row">
  <div class="col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/swiftbrush/results.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
