---
layout: distill
title: "Prompt-based image editing via Delta Denoising Score"
description:
tags: iccv2023 diffusion-model score-distillation
categories: review
giscus_comments: false
date: 2024-06-24
featured: false
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

bibliography: 2018-12-22-distill.bib
---

Score Distillation Sampling (SDS), proposed by [DreamFusion](https://arxiv.org/abs/2209.14988), can be used to generate NeRFs from an input prompt. Furthermore, by replacing the parameters of a NeRF with the pixels of an image, we can utilize SDS for prompt-based image editing, as illustrated in the figure below.

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/dds/sds_dds.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class="caption">
  Comparison of SDS and DDS for prompt-based image editing. The image is from DDS paper.
</div>

Looking at the image, it can be seen that the results of SDS are blurry and have low fidelity. Specifically, the details that are lost are those that are not mentioned in the input prompt. To explain this phenomenon, the authors of Denoising Delta Score (DDS) argue that the reason is due to the gradient $\nabla_\theta \mathcal{L}_\mathrm{SDS}$ consisting of two components.

$$
\nabla_\theta \mathcal{L}_\mathrm{SDS}=\delta_{text} + \delta_{bias}
$$

where

$\delta_{text}$ is the “good” direction that directs the image to the closest image matching the text and
$\delta_{bias}$ is the “bad” direction that causes the image to become smooth and blurry.

Based on that observation, DDS proposes a method to achieve a more optimal update direction, namely

$$
\nabla_\theta \mathcal{L}_\mathrm{DDS} \approx \delta_{text}
$$

In this post, we explore how they do that.

## SDS vs. DDS

To illustrate the distinction between SDS and DDS, consider a prompt-based image editing scenario. Given an input image described as `A turtle flying a kite`, as depicted below, our objective is to modify the image to align with the revised prompt `A turtle flying a kite at sunset`.

<div class="row">
  <div class="mx-auto col-sm-8 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/dds/example_problem.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

Suppose the set of parameters to be optimized is $\theta$, for example, $\theta$ could be the values of the pixels of input images. SDS and DDS update x in different ways:

### SDS

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/dds/sds_opt.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

$$
\nabla_\theta \mathcal{L}_\mathrm{SDS}=\mathbb{E}_{t,\epsilon} \left[
w(t)(\epsilon_\phi(\mathbf{z}_t;y,t)-\epsilon)\frac{\partial\mathbf{z}}{\partial\theta}
\right]
$$

$$
y=\unicode{x201C}\textrm{a turtle flying a kite at sunset}\unicode{x201D}
$$

### DDS

DDS requires an additional reference image for its optimization.

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/dds/dds_opt.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

$$
\nabla_\theta \mathcal{L}_\mathrm{DDS}=\nabla_\theta \mathcal{L}_\mathrm{SDS}(\mathbf{z},y) - \nabla_\theta \mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y})
$$

$$
y=\unicode{x201C}\textrm{a turtle flying a kite at sunset}\unicode{x201D}
$$

$$
\hat{y}=\unicode{x201C}\textrm{a turtle flying a kite}\unicode{x201D}
$$

## Why do DDS images look better than SDS ones?

Firstly, the gradient of $\mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y})$ is equivalent to the “bad” direction. Specifically,

$$
\nabla_\theta \mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y}) = \hat{\delta}_{bias}
$$

which can be proved visually by applying SDS optimization on a well-matched pair of image and prompt, as shown in the image below.

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/dds/bad_direction.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

Moreover, “bad” directions of closely related images are similar. Specifically,

$$
\delta_{bias} \approx \hat{\delta}_{bias}
$$

as shown by cosine similarity between SDS update directions of matched pairs.

<div class="row">
  <div class="mx-auto col-sm-6 mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/dds/cosine_sim.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

From these observations, we can conclude that

$$
\nabla_\theta \mathcal{L}_\mathrm{DDS}=\nabla_\theta \mathcal{L}_\mathrm{SDS}(\mathbf{z},y) - \nabla_\theta \mathcal{L}_\mathrm{SDS}(\hat{\mathbf{z}},\hat{y}) \approx \delta_{text}
$$

Thus, DDS update direction concentrates on editing the relevant portion of the image such that it matches the target prompt.

<div class="row">
  <div class="mx-auto col-sm-8 mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/dds/dds_grad.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

## Train Image-to-Image translation model using DDS

Editing an image by incrementally updating with DDS direction has several drawbacks:

- Require prompts for both the input and the edited image
- Inference time is long

To address these shortcomings, the authors of DDS propose a novel unsupervised training for a task-driven image-to-image model. The training process of such a model is illustrated in the figure below.

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
    {% include figure.liquid path="assets/img/dds/img-to-img.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class="caption">
  How to train a task-driven image-to-image model using DDS. The image is from DDS paper.
</div>

The idea here is that instead of directly optimizing the edited images, we optimize the weights of a neural network that outputs the edited images given the input images. As shown in the figure above, the weights of this image-to-image model are optimized using the DDS update direction. Additionally, we can make the model more versatile by conditioning it on task embeddings, allowing it to edit the input image based on the given task. To enable this, we need a suitable dataset that ensures the input task and the caption of the output image are well matched.
