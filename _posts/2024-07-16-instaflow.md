---
layout: distill
title: "InstaFlow: one step is enough for high-quality diffusion-based text-to-image generation"
description:
tags: iclr2024 diffusion-model one-step-generation
categories: review
giscus_comments: false
date: 2024-07-16
featured: false
mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

bibliography: 2018-12-22-distill.bib
---

[InstaFlow](https://arxiv.org/abs/2309.06380) introduces a method to transform a many-step diffusion model into a high-quality one-step generative model. The core concept involves applying a reflow process to straighten the flow trajectories before distillation training.

## Background

Background in Flow Matching, Rectified Flow and Reflow is helpful to fathom the idea behind InstaFlow.

### Flow Matching

The score-based model shows that diffusion process can be expressed as an ordinary differential equation (ODE) which is indirectly learned via score-matching:

$$
\mathrm{d}x = \left[ f(x,t) - \frac{1}{2} g(t)^2 \nabla_x \log p_t(x)  \right] \mathrm{d}t
$$

In a nutshell Flow matching directly learn that ODE, i.e. it learns what inside the bracket

Given $\pi_0$ as a standard Gaussian distribution and $\pi_1$ as the image data distribution. Flow matching learns to transfer $\pi_0$ to $\pi_1$ via an ODE:

$$
\frac{\mathrm{d}X_t}{\mathrm{d}t}=\mathcal{v}(X_t,t)
$$

such that if the initial $X_0 \sim \pi_0$ then $X_1 \sim \pi_1$. The velocity field $\mathcal{v}$ is learned by minimizing a simple mean square objective:

$$
\min_\mathcal{v}\mathbb{E}_{(X_0,X_1) \sim \gamma} \left[ \int_0^1 ||\frac{\mathrm{d}}{\mathrm{d}t}X_t - \mathcal{v}(X_t,t)||^2 \mathrm{d}t \right]
\tag{1}
$$

where $X_t = \phi (X_0, X_1, t)$ is an interpolation function between $X_0$ and $X_1$ that is differentiable w.r.t timestep $t$. Commonly $\phi$ has the form

$$
X_t = \phi (X_0, X_1, t)=\alpha_tX_0 + \beta_t X_1
$$

The specific choice of $\alpha_t,\beta_t$ make different flow models.

### Rectified Flow

Rectified Flow model suggests a special choice of $\phi$

$$
X_t = \phi (X_0, X_1, t)= (1-t)X_0+ tX_1
$$

which entails

$$
\frac{\mathrm{d}X_t}{\mathrm{d}t}=X_1-X_0=const
$$

As a result, the trajectory from $X_0$ to $X_1$ is contrainsted to a straight line, which is advantageous for ODE integration as it allows for the use of larger integration steps without compromising accuracy.

### Reflow

Training a rectified flow using Equation 1 alone would not lead to a satisfactorily straight flow model. Study in [https://arxiv.org/abs/2209.03003](https://arxiv.org/abs/2209.03003) suggests that we can enhance the straightness by recursively applying the rectifying procedure, as shown in the image below.

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/instaflow/reflow.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>
<div class="caption">
  Reflow procedure. The image is from <a href="https://arxiv.org/abs/2209.03003">https://arxiv.org/abs/2209.03003</a>
</div>

## InstaFlow

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/instaflow/overview.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

InstaFlowâ€™s main find is: Direct distillation fails, while reflow + distillation succeeds, from which they proposes a 2-steps method to train a one-step denoising model

### Step 1: straighten a pretrained text-conditioned model via text-conditioned reflow

Given a pretrained text-to-image models, e.g. Stable Diffusion, this step applies Reflow to improve straightness of the model as below

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/instaflow/reflow_loss.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/instaflow/reflow_alg.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

Intuitively the Reflow attempts to keep the reflowed ODE coincided with the pretrained ODE while make it as straight as possible.
The training of this step relies solely on synthetic data generated by simulating the pretrained ODE which makes pairs of $(X_0,X_1)$.

Figure 6 of the paper presents a comparison between before and after reflowing the SD 1.4 model.

<div class="row">
  <div class="mx-auto col-sm-6 mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/instaflow/reflow_example.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

It can be seen that after reflow, straightness has been significantly improved. However, there has also been a slight change in the start and end points of the trajectories. I speculate that this represents a trade-off between high straightness without intersection and consistency with the initial flow.

### Step 2: text-conditioned distillation

While improving straightness, a limited number of iterations of step 1 cannot achieve perfect linear flow, resulting in an unusable one-step generation as illustrated in Figure 7 in the paper (the middle subfigure in the bottom row). To address this, the authors propose the following distillation technique.

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/instaflow/distill_loss.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

<div class="row">
  <div class="mx-auto col-sm mt-3 mt-md-0">
      {% include figure.liquid path="assets/img/instaflow/distill_alg.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  </div>
</div>

This means learning a single Euler step mapping $X_0$ to $\mathrm{ODE}\[ \mathcal{v}_k \](X_0 \vert \mathcal{T})$ by minimizing a similarity loss $\mathbb{D}$. In the paper's experiment, the similarity loss is LPIPS loss.

Moreover, the author emphasizes that this step 2 will not be effective without step 1. The reason is that reflow has created a better coupling compared to the original flow in terms of transport costs, which makes it easier to train the student network.

## Limitations

InstaFlow has few limitations:

- High cost of synthesizing and storing the dataset
- Reliance on a synthetic training dataset
- Inferior image quality compared to the original model

These limitations have been addressed in their [subsequent work](https://arxiv.org/abs/2405.07510)
